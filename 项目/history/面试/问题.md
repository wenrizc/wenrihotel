
### **第一轮：核心 RAG 流程深度剖析 (Core RAG Pipeline)**

你在简历里提到，你设计并实现了一套“面向复杂历史文献的 RAG 流程”，里面包含了很多有意思的优化点，我们来逐一拆解一下。

1. **分块策略 (Chunking Strategy):**
    
    - 你提到了“父子文档分块策略”，这是一个很好的思路。我想了解一下你做这个技术选型时的思考。为什么传统的固定大小（Fixed-size）或递归字符（Recursive Character）分块策略不适用于你的“复杂历史文献”场景？它具体遇到了什么问题？
        
    - 请详细描述一下你的“父子文档”是如何定义的？“父文档”是自然段落、章节，还是其他形式？“子文档”呢？父与子之间的大小比例关系大概是怎样的？
        
    - 在检索时，你是如何利用这个父子结构的？是先检索到高精度的“子文档”，然后将它关联的“父文档”作为上下文提供给 LLM 吗？如果是，为什么不直接提供“子文档”？这样做是基于什么考虑？
        
    - 这个父子关联关系你是如何存储的？在向量数据库中是通过 metadata 字段关联 ID 吗？还是有其他更高效的存储和查询设计？
        
2. **查询优化 (Query Transformation):**
    
    - 你同时使用了“查询扩展”和“HyDE”技术。在你的系统里，这两者是并行生效还是串行工作的？或者是在什么情况下触发哪一个？
        
    - HyDE 的核心是生成一个假设性文档再进行向量检索。这个生成步骤本身会引入一次 LLM 调用，增加了延迟和成本。你是如何评估并接受这个 trade-off 的？在你的项目中，HyDE 带来的检索精度提升，是否可以通过量化指标证明其必要性？
        
    - “查询扩展”具体是怎么做的？是基于词库的同义词扩展，还是也利用了 LLM 进行查询重写（Query Rewriting）？如果是后者，你如何设计 prompt 来引导 LLM 做高质量的扩展，同时避免它过度发散，引入噪声？
        
3. **检索机制 (Retrieval):**
    
    - 你提到了“混合检索”，融合了关键词和向量检索。我们来聊聊实现细节。你底层的检索引擎是什么？是 Elasticsearch/OpenSearch，还是其他方案？向量检索部分用的是 FAISS、Milvus 还是其他的向量数据库？
        
    - 两种检索结果你是如何融合（Fusion）的？是简单的加权求和，还是使用了更复杂的算法，比如 Reciprocal Rank Fusion (RRF)？这个融合策略的权重或者参数，你是如何确定的？是通过实验调参得到的吗？
        
    - 对于历史文献这种场景，你认为关键词检索的不可替代性体现在哪里？能举一两个具体的例子，说明什么类型的查询是纯向量检索难以处理，而必须依赖关键词的吗？
        
4. **重排序 (Reranking):**
    
    - 你引入了重排序模型。请问你用的 Reranker 模型具体是哪一款？（比如 bge-reranker, cohere rerank 等）
        
    - 在你的流程里，从混合检索阶段召回了多少个（Top K）候选文档送入 Reranker？这个 K 值你是如何决定的？它直接影响到 Reranker 的计算开销和最终精度，你是如何平衡这两者的？
        
    - Reranker 本身有延迟。你是如何评估整个 RAG 流程的端到端延迟的？有没有做过性能分析，比如每个环节（查询优化、检索、重排序）的耗时分别是多少？瓶颈在哪里？
        
5. **整体流程串讲:**
    
    - 现在，请把上面这些点串起来。假设用户输入一个查询：“**分析一下安史之乱后，唐代中央政府对地方藩镇的控制策略演变**”。请你完整地描述一下，这个查询在你的系统中，从输入到最终生成喂给 LLM 的上下文（Context），所经历的**完整数据流和处理步骤**。
        

### **第二轮：“历史穿越”互动功能与 Agent 设计**

这个功能听起来非常创新，我们来聊聊它的实现。这部分考察的是你对复杂业务逻辑的系统设计和抽象能力。

1. **动态知识引擎与双源检索:**
    
    - 你提到了“双源信息检索”：史料库和用户过往对话历史。当一个新问题进来时，系统如何决定是应该侧重于检索史料库，还是侧重于对话历史？或者两者都检索？
        
    - 如果两者都检索，两种来源的信息如何进行融合和排序？如果史料库的客观事实与对话历史中推断的上下文产生冲突，你的系统会如何处理？有没有一个事实优先级的策略？
        
    - “对话历史”是如何被检索的？你是把每一轮对话都作为一个独立的文档进行向量化吗？这样做会不会导致上下文的割裂？有没有考虑过对对话历史进行摘要（Summarization）后再利用？
        
2. **Agent 与外部工具调用:**
    
    - 你说 Agent 会调用 CBDB、维基百科等资料库。这个“调用”决策是如何做出的？是基于大模型的 Function Calling/Tool Calling 能力吗？还是你设计了一套基于规则或关键词的触发机制？
        
    - 请你设计或描述一下用于驱动 Agent 的核心 Prompt。这个 Prompt 需要包含哪些要素，才能让 Agent 准确地理解何时、以及如何使用这些外部工具来增强上下文？
        
    - 外部 API 调用存在不确定性，比如网络超时、查询无结果、返回数据格式异常等。你的 Agent 是如何处理这些异常情况的？有重试、降级或者错误兜底的机制吗？
        

### **第三轮：工程实现与性能优化**

这部分我们关注一些更底层的工程细节和优化思路，这能体现你的工程素养。

1. **EPUB 文档解析:**
    
    - 你特别提到了对 EPUB 格式的优化。为什么选择基于 EPUB 本身的章节结构和 HTML 标签来做分块，而不是直接将其转换为纯文本再处理？相比后者，你的方法优势在哪？
        
    - 历史文献中经常有复杂的排版，比如脚注、尾注、图表、引文块等。你在基于 HTML 标签做结构化切分时，是如何处理这些特殊元素的？有没有遇到什么难点？
        
2. **推理成本与延迟优化:**
    
    - 你提到了通过“固定提示词前缀”来优化 KV-Cache 命中率。这是一个非常好的点。请详细解释一下它的原理。这个“固定的前缀”具体包含了哪些内容？是 System Prompt 吗？
        
    - 你提到的“追加式、确定性序列化更新”上下文，这里的“确定性序列化”是指什么？能否举一个例子说明？为什么“确定性”对于提高 KV-Cache 命中率至关重要？
        
    - “主动操控模型的注意力，将任务的核心目标复述到上下文末端”，这个技巧我们称之为 "Lost in the Middle" 问题的缓解策略。你是如何自动化地“复述核心目标”的？是简单地把用户的第一轮问题追加到末尾，还是通过 LLM 对多轮对话进行目标总结？
        

### **第四轮：量化评估与迭代**

一个系统的好坏最终需要数据来证明。你提到了基于 Ragas 的评估体系，这非常棒。

1. **评估体系构建:**
    
    - Ragas 提供了很多评估维度，比如 Faithfulness, Context Precision, Context Recall 等。在你的项目中，你认为**最核心**的一到两个评估指标是什么？为什么？
        
    - 这些评估指标通常需要 Ground Truth。你的 Ground Truth 是从哪里来的？是人工标注的，还是利用更强的模型（如 GPT-4）生成的？如果是后者，你如何评估和信任这个“裁判模型”的判断？
        
2. **优化方案对比:**
    
    - 你提到“系统性对比多种优化方案”。请举一个具体的例子。比如，在对比“基础分块策略”和你的“父子文档分块策略”时，你观察到 Faithfulness、Context Precision 这些指标具体发生了怎样的数值变化？
        
    - 你是否做过“有 Reranker”与“无 Reranker”的 A/B 对比实验？除了提升精度，它对其他指标（比如回答的相关性 Answer Relevancy）有什么影响？有没有发现什么意料之外的 trade-off？
        
    - 整个评估和优化的流程是怎样的？你建立了一套自动化的评估流水线（Pipeline）吗？还是每次优化后手动跑一遍评估脚本？这个流程的效率如何？
        
