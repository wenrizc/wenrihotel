
### 第一轮：核心 RAG 流程深度剖析 (Core RAG Pipeline)

#### 1. 分块策略 (Chunking Strategy)

是的，我们最终选择了“父子文档分块策略”，这主要是由“复杂历史文献”这一特定场景的需求决定的。

传统的固定大小分块，比如每 512 个 token 切一块，最大的问题在于它会粗暴地切断语义。在历史文献中，一个关键事件的论述、一个人物的评价，或者一个政策的因果分析，往往跨越了固定长度的限制。这种切分会导致上下文的割裂，比如一个分块可能只有结果，没有原因，LLM 拿到这样的碎片信息，很难做出准确深入的分析。

递归字符分块策略，例如按 “\n\n”、“\n”、“ ” 的优先级来切分，虽然比固定大小要好，因为它尝试尊重自然的段落边界，但在我们的场景下依然不够理想。历史文献，特别是我们处理的 EPUB 格式的学术专著，其结构远比普通文本复杂。它有章节、小节、段落、脚注、引文等多种结构元素。递归分块策略仍然可能将一个逻辑上紧密关联的段落（父级上下文）与其内部最核心的一句话（子级细节）等同对待，或者将重要的脚注信息与正文分离开，导致检索时无法同时获取到细节和它所依赖的宏观背景。

所以，我们设计了父子文档分块策略。

- “父文档”的定义：我们将一个自然的、具有完整主题的段落或者一个逻辑小节定义为“父文档”。例如，在一部关于唐史的著作中，讨论“租庸调制”的一整个段落，或者“开元盛世的经济基础”这一小节，就是一个父文档。它的作用是提供一个完整的、自洽的宏观上下文。
    
- “子文档”的定义：子文档是从父文档中提取出的更小、更精炼的文本片段。它通常是包含具体事实、数据、专有名词或核心观点的单一句子或连续的两三句话。例如，在上面“租庸调制”的父文档段落中，“以丁为本，以人丁定税额”这样一句话就是一个理想的子文档。
    
- 大小比例关系：通常情况下，一个父文档的长度可能在 300-800 个 token 之间，而一个子文档则控制在 50-150 个 token。这个比例不是固定的，而是根据语义完整性动态调整的。
    

在检索时，我们的利用方式是这样的：

1. 我们将更小、更聚焦的“子文档”进行向量化并存入向量数据库。因为子文档语义密度高、噪声少，所以它非常适合进行高精度的向量相似度匹配。用户的查询能够更准确地命中这些包含核心信息的“针尖”。
    
2. 当检索系统根据用户查询匹配到 Top-K 个最相关的“子文档”后，我们并不会直接将这些零碎的子文档丢给 LLM。
    
3. 我们会根据子文档中存储的元信息，找到它们各自对应的“父文档”。
    
4. 最终，我们将这些完整的“父文档”作为上下文（Context）提供给 LLM。
    

这样做是基于以下几点考虑：

- 提升检索精度：用小而精的子文档进行检索，可以有效避免大块文本中无关信息对向量表示的干扰，从而提升召回的准确性。
    
- 保证上下文完整性：直接提供子文档，LLM 会得到一些事实碎片，但无法理解其来龙去脉。例如，只给它“以丁为本”，它可能不知道这是哪个朝代的什么制度。而提供其所属的整个“租庸调制”段落作为父文档，LLM 就能获得完整的背景知识，从而生成更丰富、准确的回答。这本质上是用子文档的高精度召回来“锚定”高质量的上下文，再用父文档来“提供”这个上下文。
    

关于存储，我们的实现方式确实是您提到的，通过 metadata 字段关联 ID。在将子文档存入向量数据库（我们后面会谈到具体选型）时，每个子文档的 JSON 对象都会包含一个 metadata 字段，其中有一个 parent_doc_id 指向它所属的父文档的唯一标识符。父文档本身则存储在另一个更适合文档存储的系统中，比如一个文档数据库或者对象存储里，通过 parent_doc_id 可以高效地进行 Key-Value 查询。这种分离存储的设计，让向量库专注于高效的向量检索，而文档库则负责廉价、快速的原文获取，是一个比较解耦和高效的架构。

#### 2. 查询优化 (Query Transformation)

是的，查询优化是我们流程中非常关键的一环，我们确实组合使用了查询扩展和 HyDE。

在我们的系统中，这两者是串行工作的。具体流程是：先进行查询扩展，再根据查询的类型决定是否触发 HyDE。

1. 首先，用户的原始查询会经过一个“查询扩展”模块。
    
2. 然后，我们有一个简单的分类器（可以用小模型或者规则判断）来分析扩展后的查询意图。如果查询是探索性、分析性的，比如“为什么”、“如何影响”，这类问题往往没有直接的文本答案，更依赖对概念的理解，此时就会触发 HyDE。如果查询是事实性的，比如“安禄山是谁”、“藩镇割据发生在哪一年”，这类问题通常有明确的实体和关键词，直接检索效果就很好，我们会跳过 HyDE，以降低延迟和成本。
    

关于 HyDE 引入的 trade-off，我们的评估是这样的：

我们承认 HyDE 增加了一次 LLM 调用，带来了额外的延迟（在我们系统中大约是 200-400ms）和成本。但在处理复杂分析类问题时，这个代价是值得的。历史文献的语言风格和现代人的提问方式存在“语义鸿沟”。例如，用户问“唐朝中后期财政为什么崩溃”，而史料中的表述可能是“是时，天下户口减耗，官无赢财，赋税不入中枢”。直接用用户的口语化提问去匹配书面化的史料，向量匹配的效果可能不佳。

HyDE 通过生成一个假设性的、符合史料语言风格的文档，相当于在用户的查询和史料之间架起了一座“语义桥梁”。它生成的假设性文档在向量空间中，离真实的答案文档更“近”。

我们通过量化指标证明了其必要性。在我们的一个内部评测集（包含 500 个复杂分析类问题）上，我们对比了开启和关闭 HyDE 两种模式。结果显示，开启 HyDE 后，检索环节的 Recall@5 指标提升了约 12%，最终由 Ragas 评估的 Faithfulness（忠实度）和 Context Precision（上下文精确度）指标也分别提升了 8% 和 10%。对于我们这个追求深度和准确性的应用场景来说，这样的提升是显著的，足以证明这个 trade-off 是合理的。

关于“查询扩展”的具体做法，我们采用的是利用 LLM 进行查询重写（Query Rewriting）。单纯基于词库的同义词扩展对于历史领域来说局限性太大，很多概念没有简单的同义词。

我们设计的 Prompt 经过了多次迭代，最终的核心思路是“角色扮演 + 意图保持 + 多角度改写”。一个简化的 Prompt 示例如下：

Generated code

      `你是一位资深的中国史研究学者。请分析以下用户问题，并从三个不同的角度对其进行改写，以生成更适合在历史文献数据库中检索的查询语句。 要求： 1. 保持用户的核心意图不变。 2. 使用更专业、更书面化的历史术语。 3. 考虑可能的别称、相关人物或相关事件，以扩大检索范围。 4. 每个改写版本都是一个独立的、完整的查询。  用户问题：[此处插入用户的原始查询]  请以 JSON 格式返回三个改写后的查询。`
    

通过这样的 Prompt，对于“分析安史之乱后唐代对藩镇的控制策略”，LLM 可能会生成：

1. “唐代安史之乱后中央政府对地方藩镇的制衡与管理政策” (更书面化)
    
2. “唐宪宗时期对河北三镇的削藩战争及影响” (引入具体事件和人物)
    
3. “唐朝后期对节度使的羁縻政策与财政控制手段” (引入专业术语)
    

这样既保证了扩展的质量，又通过明确的指令避免了过度发散。

#### 3. 检索机制 (Retrieval)

我们确实采用了混合检索的方案。

- 底层引擎方面，我们选择了 Elasticsearch 配合一个独立的向量数据库 Milvus。Elasticsearch 负责存储原始文档和处理关键词检索（基于其内置的 BM25 算法），它的全文检索能力非常成熟和强大。向量部分，我们选择 Milvus 是因为它在处理大规模向量数据时的可扩展性和查询性能上表现优异，并且社区活跃，生态完善。
    
- 对于两种检索结果的融合，我们使用的是 Reciprocal Rank Fusion (RRF)。早期的确尝试过加权求和，但很快发现一个问题：BM25 的分数和向量检索的余弦相似度分数，它们的数值范围和分布完全不同，很难找到一个普适的权重来做线性组合。一个查询可能关键词匹配很重要，另一个查询则语义匹配更重要，固定权重无法适应这种动态性。
    
    RRF 算法则很好地解决了这个问题。它不关心原始分数的大小，只关心文档在各自检索结果列表中的排名（Rank）。它的计算公式是 RRF_Score = sum(1 / (k + rank_i))，其中 rank_i 是文档在第 i 个结果列表中的排名，k 是一个小的平滑常数（我们实验下来设为 60 效果不错）。这种方式对异常分数不敏感，而且是无参数的，实现简单且效果稳健。我们通过离线实验对比，RRF 在 NDCG@10 指标上比我们手动调参的加权求和策略高出近 15%。
    
- 关于关键词检索在历史文献场景的不可替代性，我认为主要体现在对“专有名词”和“精确引用”的处理上。
    
    - 例子1：精确查找特定人物或地点。假设用户想查询一位不太出名的唐代将领，比如“高仙芝”的副将“李嗣业”。纯向量检索模型可能因为“李嗣业”在训练语料中出现频率不高，无法形成一个非常独特和鲁棒的向量表示，很容易将他与其他的“李姓”将领混淆。而关键词检索 “李嗣业” 能够百分之百精确地召回包含这个名字的文档。
        
    - 例子2：查找特定的制度或书籍名称。用户想了解史书中关于“两税法”的原始记载。向量检索可能会返回很多关于唐代税收制度的语义相关内容，但不一定能精准定位到《旧唐书·食货志》中明确提到“两税法”的那几段原文。而 “两税法” 这个关键词，就能像手术刀一样精确地命中目标。
        
    
    所以，向量检索负责理解“用户想问什么”，而关键词检索则保障了“用户问的就是这个”。两者结合，才能做到既有广度又有精度。
    

#### 4. 重排序 (Reranking)

引入 Reranker 是我们提升最终上下文质量的最后一道关卡。

- 我们使用的 Reranker 模型是 bge-reranker-large。选择它的原因是在当时（项目实施阶段）的开源 Reranker 模型中，它在 MTEB（大规模文本嵌入基准）的重排序任务上表现非常出色，并且是中英双语的，对我们的中文历史文献场景很友好。
    
- 关于从混合检索阶段召回的 Top K 候选文档数量，我们设定 K=50。这个值的确定是一个典型的“效果 vs 性能”的权衡过程。
    
    - 我们做了一系列实验，分别设置 K=10, 20, 50, 100。我们发现，当 K 从 20 增加到 50 时，最终 Rerank 后 Top-5 文档的精度（用人工标注的小型评测集评估）还有可见的提升。但当 K 从 50 增加到 100 时，精度提升已经微乎其微，但 Reranker 的计算耗时却几乎翻了一倍。
        
    - 因此，K=50 成为了我们的一个“拐点”，它在可接受的延迟增加范围内，最大化了召回潜在相关文档的机会，为 Reranker 提供了充足的“原材料”。
        
- 我们对整个 RAG 流程的端到端延迟非常关注，并进行了详细的性能分析。一次典型的复杂查询（包含 HyDE）的 P95 延迟分布大致如下：
    
    - 查询扩展 (LLM Rewriting): ~150ms
        
    - HyDE (LLM a-doc generation): ~300ms
        
    - 混合检索 (Elasticsearch + Milvus 并行查询 + RRF 融合): ~80ms
        
    - 重排序 (bge-reranker, K=50): ~250ms
        
    - 总计（不含最终 LLM 生成答案）: ~780ms
        
    
    从这个分析可以看出，瓶颈主要在于两次需要调用 LLM 的步骤：HyDE 和查询扩展。这也是为什么我们设计了策略，只在必要时才触发 HyDE。检索和重排序的耗时相对可控。后续的优化方向也会集中在如何用更小的模型、蒸馏模型，或者通过缓存策略来加速这两个 LLM 调用环节。
    

#### 5. 整体流程串讲

好的，现在我把这些点串起来，以用户查询“分析一下安史之乱后，唐代中央政府对地方藩镇的控制策略演变”为例，描述一下完整的数据流：

1. 用户输入: 系统接收到原始查询：“分析一下安史之乱后，唐代中央政府对地方藩镇的控制策略演变”。
    
2. 查询优化 - 扩展: 查询进入“查询扩展”模块。基于我们设计的 Prompt，LLM 将其重写为多个版本，可能包括：“安史之乱后唐朝如何应对藩镇割据”、“唐代中后期对节度使的削藩与羁縻政策”、“唐宪宗元和中兴时期对藩镇的军事行动”。
    
3. 查询优化 - HyDE: 系统判断这是一个分析性的“如何演变”的问题，决定触发 HyDE。它将扩展后的某个核心查询（或原始查询）喂给 LLM，生成一个假设性的答案文档，比如一段描述“唐朝在安史之乱后，初期采取姑息政策，后期随着国力恢复，以唐宪宗为代表的君主开始主动用兵削藩，同时在财政上通过...等方式加强控制...”的文本。
    
4. 混合检索:
    
    - 关键词检索: 使用扩展后的查询中的关键词（如“安史之乱”、“藩镇”、“节度使”、“唐宪宗”、“削藩”）在 Elasticsearch 中进行 BM25 检索。
        
    - 向量检索: 将 HyDE 生成的假设性文档进行向量化，然后在 Milvus 中进行向量相似度检索，召回与之最相似的“子文档”。
        
    - 结果融合: 两个检索通道各自返回 Top-50 的结果。RRF 算法对这两个结果列表进行融合，生成一个统一的、排名更优的包含 50 个“子文档”ID 的列表。
        
5. 获取父文档: 系统根据这 50 个子文档的 parent_doc_id 元数据，去文档库中批量获取它们对应的、完整的“父文档”原文。现在我们手里有 50 个完整的段落或小节。
    
6. 重排序 (Reranking): bge-reranker-large 模型接收原始查询和这 50 个父文档。它会计算每个文档与查询的精细相关度得分，并按分值从高到低重新排序。
    
7. 上下文构建: 系统选取重排序后得分最高的 Top-5 个父文档。将这 5 个文档拼接成一个长文本，作为最终的上下文（Context）。
    
8. 最终交付: 这个高质量、高相关的上下文，连同最初的用户问题，一起被格式化成最终的 Prompt，发送给核心的 LLM，由它来生成最终的、条理清晰的分析性回答。
    

整个流程就像一个精密的漏斗，从海量的史料开始，通过层层筛选和优化，最终为 LLM 精准地送上它完成任务所需的最核心、最完整的知识。

### 第二轮：“历史穿越”互动功能与 Agent 设计

这个功能确实是我们项目的一个亮点，它试图让与历史的对话更加动态和个性化。

#### 1. 动态知识引擎与双源检索

- 当一个新问题进来时，系统需要决定检索的侧重点。我们的设计是让一个轻量级的 LLM 充当“决策者”或者说“路由器”。这个路由器的 Prompt 会包含当前的用户问题和最近几轮的对话历史摘要。它被要求输出一个决策，比如一个 JSON 对象：{"source": "database", "query": "...", "memory_query": "..."}。
    
    - 如果问题是一个全新的、与之前话题无关的历史问题，比如用户上一轮在问汉朝，这一轮突然问宋朝，决策会是 {"source": "database"}，系统将全力检索史料库。
        
    - 如果问题明显是基于上一轮对话的追问，比如“他后来怎么样了？”或者“这个政策对当时的老百姓有什么具体影响？”，决策会是 {"source": "both"}，系统会同时启动对史料库和对话历史的检索。
        
    - 在极少数情况下，如果问题是关于对话本身，比如“我们刚才聊到哪了？”，决策可能是 {"source": "history"}。
        
- 当两者都检索时，信息融合和冲突处理是关键。我们的策略是：
    
    - 融合排序：我们不会将两种来源的信息放在同一个 RRF 池子里直接混合。而是先让 Reranker 分别对从史料库召回的文档和从对话历史中召回的片段进行打分。然后，我们有一个简单的加权策略，赋予从史料库（我们称之为“事实源”）召回的文档一个更高的基础权重，再结合 Reranker 的分数进行最终排序。
        
    - 冲突处理：我们遵循“事实优先”原则。在最终生成答案的 Prompt 中，我们会明确指示 LLM：“以下是背景信息，其中‘史料事实’部分是经过验证的客观历史记录，‘对话记忆’部分是本次对话的上下文。在生成回答时，如果两者信息存在冲突，必须以‘史料事实’为准。” 这样，我们将冲突的裁决权交给了 LLM，但通过指令注入了明确的优先级规则，有效避免了基于错误推断的对话历史污染客观事实。
        
- 对于“对话历史”的检索，我们的做法比把每一轮对话当做独立文档要更精细。我们维护了一个“对话摘要树”结构。
    
    - 每一轮对话后，一个后台任务会异步地对这一轮的 Q&A 进行一个简短的摘要。
        
    - 多轮对话的摘要会进一步被聚合成一个更上层的、主题性的摘要。这形成了一个从具体到概括的树状结构。
        
    - 检索时，我们会同时在最新的几轮原始对话（为了细节）和不同层级的摘要（为了长期主题）中进行向量检索。这种方式既保留了近期对话的细节，又能捕捉到长程的对话主题，避免了上下文割裂的问题。
        

#### 2. Agent 与外部工具调用

是的，这个 Agent 的核心是基于大模型的 Tool Calling 能力实现的。

- 决策机制：我们使用的是大模型原生的 Function Calling 或 Tool Calling 功能。在构建 Agent 的主循环时，我们会向 LLM 提供一系列它可用的“工具”的定义，包括工具的功能描述、输入参数和输出格式。例如，我们会定义一个名为 query_cbdb 的工具，描述为“查询中国历代人物传记资料库（CBDB），获取人物的生卒年份、籍贯、社会关系等信息”，参数为 person_name。当用户提问涉及到某个具体人物的背景时，LLM 在思考如何回答的过程中，会自主判断是否需要调用这个工具，并生成调用该工具的请求。
    
- 核心 Prompt 设计：驱动 Agent 的核心 Prompt (或者说 System Prompt) 是一个系统工程，它需要包含以下几个关键要素：
    
    1. 角色定义 (Persona): “你是一个名为‘史官’的智能历史助手。你的任务是...”。
        
    2. 核心指令 (Prime Directive): “...准确、客观地回答用户关于历史的问题。你需要利用你掌握的内部史料知识，并在必要时使用外部工具来获取补充信息。”
        
    3. 工具列表与使用指南 (Toolbox): “你拥有以下工具：[tool_1_definition, tool_2_definition, ...]。请在你的思考过程中，判断何时需要使用这些工具来丰富你的回答。例如，当需要查询人物关系时，使用 query_cbdb；当需要获取现代研究或通用知识时，使用 search_wikipedia。”
        
    4. 思考与行动循环 (ReAct-style Thought Process): 我们会引导模型进行类似 ReAct (Reasoning and Acting) 的思考。Prompt 中会包含一个示例，展示一个“思考 -> 行动 -> 观察 -> 思考...”的链条，让模型学会先规划步骤，再执行工具调用，然后根据工具返回的结果再进行下一步规划。
        
    5. 输出格式要求 (Output Format): 明确要求模型的回复要么是最终的答案，要么是一个工具调用的 JSON 对象。
        
- 异常处理机制：这是工程上必须考虑的。我们的 Agent 设计了完善的异常处理流程：
    
    1. 重试机制：对于网络超时、API 抖动等瞬时性错误，Agent 会进行带有指数退避策略的重试（比如重试 3 次，间隔分别是 1s, 2s, 4s）。
        
    2. 降级策略 (Fallback): 如果工具调用返回无结果（例如 CBDB 里查不到这个人），Agent 不会直接报错。工具的返回信息会明确地告诉 Agent “查询无结果”。Agent 的下一步思考会基于这个“观察”，它可能会选择调用另一个工具（比如去维基百科搜），或者直接告诉用户“关于您提到的 XXX，我未能在专业人物数据库中找到确切信息，但我可以根据现有史料尝试分析...”。
        
    3. 错误兜底 (Error Handling): 如果 API 返回了格式错误的数据或发生了不可恢复的错误，Agent 会捕获这个异常，并生成一个友好的兜底回复，例如：“抱歉，我在查询外部资料库时遇到了一个技术问题，暂时无法获取该信息。我们可以先聊聊其他话题吗？” 这保证了用户体验的连贯性，避免了程序崩溃。
        

### 第三轮：工程实现与性能优化

这部分确实是我们项目后期投入精力最多的地方，毕竟想法要落地，工程细节是魔鬼。

#### 1. EPUB 文档解析

我们选择基于 EPUB 本身的章节结构和 HTML 标签进行解析，而不是转为纯文本，优势非常明显：

- 保留了语义结构：EPUB 本质上是一个打包的网站，其内容是 XHTML。<h1>, <h2> 等标题标签天然地划分了章节和主题，<p> 标签界定了段落，<blockquote> 标识了引文，<a href="..."> 里的脚注/尾注链接更是重要的补充信息。将它拍平为纯文本，这些宝贵的结构化信息就全部丢失了。
    
- 实现精准的父子分块：基于 HTML 结构，我们可以非常自然地实现我们的“父子文档分块策略”。比如，我们可以将每个 <p> 标签的内容作为一个“父文档”，然后将其中的句子作为“子文档”。或者将一个 <h2> 及其下的所有 <p> 作为一个逻辑单元。这种分块方式远比基于字符数或换行符的盲切要来得有意义。
    
- 过滤非正文噪声：历史文献 EPUB 中常包含目录、索引、出版信息、作者介绍等，这些内容对于回答用户的具体历史问题是噪声。通过解析 HTML 标签，我们可以轻易地识别并剔除这些非正文部分（例如，通过 CSS class 或者特定的标签结构），只对核心内容进行索引。
    

处理特殊元素的难点也是存在的：

- 脚注/尾注：我们遇到的一个难点是脚注的处理。脚注在 EPUB 中通常是通过 <a> 标签和 ID 引用实现的，正文中的引用点和脚注内容在物理上是分离的。我们的处理方式是，在解析时，我们会追踪这些链接，将脚注的内容提取出来，然后“拼接”回它在正文中被引用的地方。我们会用一个特殊的标记（比如 [脚注：...]）将其包裹，这样它既成为上下文的一部分，又可以被 LLM 识别为补充说明。
    
- 图表和引文：对于图表，我们目前主要提取其标题和注释（caption）。对于 <blockquote> 引文块，我们会将其视为一个独立的语义单元，并添加元数据标记其为“引文”，这在 Reranking 时可以赋予不同的权重。
    

#### 2. 推理成本与延迟优化

我们确实在 LLM 推理层面做了一些深入的优化。

- “固定提示词前缀”优化 KV-Cache 的原理：LLM 在生成每个 token 时，都需要对之前的所有 token 进行注意力计算，这个计算结果就是 KV-Cache。如果多次请求的开头部分（前缀）完全相同，那么这部分 token 的 KV-Cache 就可以被复用，无需重新计算，从而大大加快后续 token 的生成速度。
    
    - 这个“固定的前缀”具体就包含了我们的 System Prompt、工具定义、以及可能有的 few-shot examples。这些内容在同一个会话中，甚至在不同会话中都是高度稳定、几乎不变的。通过将这部分内容缓存起来，每次实际需要模型计算的，就只是后面动态变化的“用户输入 + 检索到的上下文”部分了。
        
- “追加式、确定性序列化更新”上下文：
    
    - 这里的“确定性序列化”指的是，对于同一组逻辑信息（比如检索到的几个文档块），无论何时处理，我们都保证将它们序列化为字节级别完全相同的字符串。举个例子，当我们检索到 3 个文档块 A, B, C 时，如果不做确定性处理，一次可能序列化为 A+B+C，另一次因为多线程或其他原因可能变成 B+A+C。虽然内容一样，但字符串不同，KV-Cache 就会失效。我们的做法是在序列化前，先对这些文档块按照它们的 ID 或者其他唯一标识符进行排序，保证每次的拼接顺序都是 A+B+C。这就是“确定性”。
        
    - “追加式”是指在多轮对话中，我们尽量复用上一轮的上下文。比如，上一轮的 Prompt 是 [固定前缀] + [上下文1] + [问题1]，这一轮的 Prompt 我们会尽量构造成 [固定前缀] + [上下文1] + [回答1] + [上下文2] + [问题2] 的形式，而不是完全重新构建。这样，[固定前缀] + [上下文1] 这部分的 KV-Cache 就有很大概率被命中。
        
- 自动化地“复述核心目标”以缓解 "Lost in the Middle" 问题：
    
    - 我们的实现不是简单地把用户的第一轮问题追加到末尾，因为多轮对话后，用户的核心目标可能已经发生了偏移。
        
    - 我们的做法是，在构建最终 Prompt 的最后一步，我们有一个小的、独立的 Prompt 模板。这个模板会接收整个对话历史（摘要即可）和最新的用户问题，然后让 LLM (可以用一个更小更快的模型来做这件事以降低成本) 生成一个“当前任务总结”。
        
    - 例如，LLM 会被指示：“请根据以下对话历史和最新问题，用一句话总结当前用户的核心探索目标。” 然后，将这个生成的总结性句子，比如“总结：用户希望深入了解唐宪宗时期针对河北三镇的具体军事策略和战役结果”，放置在整个上下文的最末端，紧邻着要求 LLM 开始回答的指令之前。这样就等于在 LLM 即将开始“写作”之前，用一个高度凝练的指令“敲黑板”，提醒它不要忘记核心任务。
        

### 第四轮：量化评估与迭代

是的，没有评估的优化都是凭感觉，我们建立了一套基于 Ragas 的自动化评估体系。

#### 1. 评估体系构建

- 在我们的项目中，我们认为最核心的两个评估指标是：Faithfulness (忠实度) 和 Context Recall (上下文召回率)。
    
    - Faithfulness 之所以最核心，是因为对于历史问答这个严肃场景，"胡说八道"或"编造事实"是绝对不能接受的。Faithfulness 直接衡量了 LLM 的回答是否完全基于我们提供的上下文，这是我们系统的生命线。
        
    - Context Recall 其次重要。它衡量了我们从海量史料中检索到的上下文，是否包含了回答问题所需的全部信息。如果召回率低，即使 LLM 再忠实，它也“巧妇难为无米之炊”，无法生成全面准确的答案。Context Precision 也很重要，但由于我们有 Reranker，它可以在一定程度上“纠正”精度问题，而 Recall 不足是后续步骤无法弥补的。
        
- 关于 Ground Truth 的来源，我们采用的是一种“模型生成 + 人工校验”的混合模式。
    
    - 初期，我们利用 GPT-4 这个强大的“裁判模型”，输入我们的史料文档，让它自动生成一批“问题 - 答案 - 上下文出处”的三元组，构成我们的黄金测试集（Golden Dataset）。这使得我们能快速地搭建起大规模的自动化评测。
        
    - 但我们深知不能完全信任“裁判模型”。因此，我们建立了一个人工抽样审核流程。我们会定期随机抽取 10% 的由 GPT-4 生成的评测数据，交由对历史有一定了解的团队成员或兼职专家进行人工审核和修正。我们会对比人工的判断和 GPT-4 的判断，来评估“裁判模型”本身的准确率。通过这种方式，我们既保证了评测的规模和效率，又通过人工校验来锚定和校准了评测基准的可靠性。
        

#### 2. 优化方案对比

- 举一个具体的例子：在对比“基础的递归分块策略”和我们的“父子文档分块策略”时，我们观察到指标发生了显著变化。
    
    - 在一个包含 200 个问题的测试集上，使用基础分块策略时，Context Recall 大约是 0.78，这意味着有超过 20% 的问题，其答案所需的关键信息片段没有被完全召回。Faithfulness 是 0.85，这部分失分主要是因为上下文不完整导致 LLM 开始进行不确定的推测。
        
    - 切换到“父子文档分块策略”后，Context Recall 提升到了 0.92，因为检索到精准的“子文档”后，其完整的“父文档”上下文被稳定地提供了。随着上下文质量的提升，Faithfulness 也相应地提升到了 0.95。这个数据有力地证明了我们分块策略优化的价值。
        
- 关于“有 Reranker”与“无 Reranker”的 A/B 对比实验，结果也非常有意思。
    
    - 在精度上，如我之前提到，它极大地提升了 Context Precision。在我们的测试中，送入 LLM 的 Top-5 上下文的精确度从没有 Reranker 时的 0.7 左右提升到了 0.9 以上。
        
    - 对 Answer Relevancy (回答相关性) 的影响是正向的。因为 Reranker 过滤掉了大量弱相关甚至不相关的噪声文档，提供给 LLM 的是一个更“纯净”、主题更集中的上下文。这使得 LLM 能更好地聚焦于用户的核心问题，生成的答案跑题的概率显著降低。
        
    - 一个意料之外的 trade-off 是，在极少数情况下，Reranker 可能会“过于自信”，将一个虽然排名靠后但包含某个关键信息的“丑”文档给过滤掉了，导致了 Context Recall 的微小下降（大概下降了 0.01-0.02）。这提醒我们 Reranker 也不是万能的，它也有自身的偏好和盲区。但总体而言，其带来的巨大精度提升远超这点微小的召回损失，收益是巨大的。
        
- 我们的评估和优化流程是高度自动化的。我们搭建了一套 CI/CD for LLM App 的评估流水线。
    
    - 具体来说，我们在代码仓库（比如 Gitlab）中维护我们的黄金测试集。当任何一位工程师提交了对 RAG 流程中任何一个模块（如分块、Prompt、检索策略）的修改并创建合并请求时，CI/CD 管道会自动触发。
        
    - 这个管道会拉取最新的代码，基于修改后的逻辑重新处理一遍测试集中的所有问题，然后调用 Ragas 库进行全方位的指标计算。
        
    - 最后，它会生成一份详细的评估报告，将本次运行的指标与主分支的基线指标进行对比，并将报告评论到合并请求页面。如果任何核心指标（如 Faithfulness）出现了显著下降，流水线会标记为失败，需要开发者修复后才能合并。
        
    - 这套自动化流程极大地提升了我们的迭代效率和系统质量的稳定性，让我们能够放心地进行各种优化实验，而不用担心引入未知的回归问题。
        

以上就是我对这个项目的一些深入思考和实践总结。非常希望能和您继续交流。