
### **一、 多级缓存架构优化**

#### **1.1 核心设计原则**

*   **数据分类与分级：** 依据数据的重要性、实时性、访问频次，定制化缓存策略与一致性模型。
*   **性能与一致性权衡：** 核心交易数据（如库存、价格、支付）优先强一致性；查询展示类数据适当放宽实时性以换取性能。
*   **故障隔离与降级：** 缓存系统具备故障隔离能力，极端情况下可降级，保障核心业务。
*   **可观测性：** 建立完善监控体系，实时追踪缓存状态、命中率、延迟等关键指标。

#### **1.2 缓存层次与职责**

维持三级缓存结构，优化职责与交互：

*   **L1 本地缓存 (Caffeine):**
    *   **定位：** 应用内高速缓存，减轻分布式缓存压力，加速热点数据访问。
    *   **内容：** 频繁访问、体积小、允许短暂不一致的热点数据副本（如热点商品只读信息、配置）。
    *   **特性：** 纳秒级访问，容量有限，应用实例间不共享。
*   **L2 分布式缓存 (Redis):**
    *   **定位：** 主力缓存层，承载大部分读请求，实现数据共享与应用解耦。
    *   **内容：** 热点数据、普通查询数据、会话、分布式锁、计数器、排行榜、消息队列（辅助）。
    *   **特性：** 毫秒级性能，丰富数据结构，持久化与高可用。
*   **L3 数据源 (MySQL):**
    *   **定位：** 数据的最终存储与一致性保障。

#### **1.3 数据一致性保障策略**

**1.3.1 多层数据一致性模型**

*   **强一致性层 (核心交易数据):**
    *   **场景：** 商品**库存** (下单扣减)、商品**价格** (下单结算)、**支付状态**、用户账户余额。
    *   **策略与实现：**
        1.  **库存 (下单扣减时):**
            *   **写 (扣减):** **强制 Redis + Lua 原子扣减。** Lua内先校验再扣减。数据库库存为最终准确值，通过Canal+MQ异步同步至Redis作预扣减上限/参考，但**下单实时扣减完全依赖Redis原子性。**
            *   **数据库同步：** 订单创建成功后，异步（或事务末尾，注意时长）更新DB库存。MQ异步更新需补偿机制（如订单与库存流水核对）。
            *   **读 (校验库存):** 直接查询Redis中Lua维护的库存。
        2.  **价格 (下单结算时):**
            *   DB变更时，Canal+MQ **同步且可靠地**更新到Redis特定key (如 `product:price:{productId}` )。
            *   下单时，直接从Redis读取。MQ处理需重试、死信队列，确保价格成功更新。
        3.  **支付状态、用户账户余额：**
            *   **写：** 必须先保证DB操作成功。
            *   **读：**
                *   支付状态：DB更新后，Canal+MQ **同步且可靠地**更新Redis。查询优先Redis。
                *   余额：高并发且允许极短延迟，可考虑可靠同步Redis并用Lua操作。否则，关键校验与扣减仍直连DB。
            *   **核心：** 此类数据“写后读”，若要求最新数据，要么读库，要么确保写后**同步阻塞式**更新/失效缓存。

*   **最终一致性层 (查询展示类数据):**
    *   **场景：** 商品详情、店铺信息、用户信息、评论列表、热门博客。
    *   **策略：** 允许短时不一致，依赖Redis作查询主力，DB作数据源与最终一致性保证。
    *   **实现与补偿机制 (优化):**
        1.  **主动更新/失效为主 (Canal + MQ -> Redis -> Caffeine):**
            *   DB变更 -> Canal监听binlog -> RabbitMQ异步通知。
            *   **应用实例消费MQ：直接更新/删除Redis缓存。**
            *   **Caffeine本地缓存：通过订阅Redis的key变更消息 (keyspace notifications 或自定义消息) 失效对应本地缓存。** (统一由Redis变更驱动，简化应用 逻辑)
        2.  **读时修复 (Read-Through with Repair - Redis优先):**
            *   查询流程：Caffeine -> **Redis** -> DB。
            *   Redis未命中或逻辑过期，则从DB加载，**回填Redis**，并间接触发Caffeine更新。
        3.  **版本号/时间戳机制 (应用于Redis缓存对象):**
            *   Redis缓存对象中包含版本号/时间戳。
            *   **MQ消费者更新Redis时：** 比较版本号/时间戳，仅当消息中版本更新时才操作Redis，防延迟消息覆盖。
        4.  **定时校对与修复 (简化，针对性增强):**
            *   **不再大规模、高频比对Redis与DB。**
            *   **策略调整：**
                *   **低频全量校对 (可选，兜底)：** 核心展示数据可保留极低频（如每日低峰）全量/关键字段校对。
                *   **错误/不一致监控与告警驱动修复：** 监控MQ处理、业务逻辑中发现不一致现象 -> 日志+告警 -> **针对性数据修复**。
                *   **关键业务流程抽样检查：** 随机抽样验证多端数据一致性。

**1.3.2 数据同步链路优化 (解决Canal延迟)**

*   **主链路 (Canal + RabbitMQ -> 应用实例):**
    *   **Canal优化：** Server性能，订阅必要表/字段。
    *   **RabbitMQ优化：** 高效序列化 (Protobuf, Kryo)，合理消费者并发，可靠消息确认 (Ack)，死信队列 (DLX)，消费者幂等性。
    *   **应用内处理：** 快速定位并失效/更新Caffeine和Redis。Caffeine可通过Redis Pub/Sub或自定义MQ广播通知。
    *   **延迟目标：** 力争平均延迟控制在 **50-100毫秒**。

*   **备用链路 (定时校对 - XXL-Job):**
    *   **兜底机制：** `CacheMaintenanceJob.java` 等任务负责最终一致性。
    *   **补偿逻辑：** 以DB为准更新Redis，并通知清理Caffeine。
    *   **监控告警：** 不一致超阈值时告警。

*   **实时链路:**
    *   **写后失效 (推荐)：** DB更新后，立即发消息/调用失效Redis和Caffeine。
    *   **回滚机制：** DB成功但缓存操作失败，记录日志并重试（如 `@Retryable`），极端情况依赖定时校对。

#### **1.4 过期策略与锁机制优化**

**1.4.1 分层与精细化过期策略**

*   **Caffeine 本地缓存 (`LOCAL_CACHE_EXPIRE_SECONDS`):**
    *   **过期时间：** 30-60秒 (核心热点如热门门票可15-30秒，普通45-60秒)。
    *   **随机偏移：** 基础过期时间上增加小随机值 (10%-20%)，防集中失效。
    *   **容量控制：** 最大20,000条，LRU/LFU淘汰。

*   **Redis 分布式缓存:**
    *   **热点数据 (`HOT_CACHE_TTL`):**
        *   **策略：** **逻辑过期**为主 (`setWithLogicalExpire`, `getFromLogicalCache`)。
        *   **过期时间：** 基础逻辑过期30分钟，加较大随机偏移 (如0-5分钟)。
    *   **普通数据 (`NORMAL_CACHE_TTL`):**
        *   **策略：** **物理过期** (`setWithRandomExpire`)。
        *   **过期时间：** 基础物理过期10分钟，加随机偏移 (如0-2分钟)。
    *   **空值缓存 (防缓存穿透 - `CACHE_NULL_TTL`):**
        *   **策略：** 物理过期 (`setCacheNull`)。
        *   **过期时间：** 较短，例如60秒。

**1.4.2 分布式锁优化设计 (解决缓存击穿与并发重建)**

*   **锁 Key 设计 (针对缓存重建):**
    *   **格式：** `lock:cache_rebuild:{businessType}:{uniqueId}` (如 `lock:cache_rebuild:ticket:123`)。
    *   **原则：** 每缓存键重建对应独立锁，实现细粒度控制。

*   **锁的粒度与类型：**
    *   **缓存重建锁：** 上述格式，解决热点过期时单线程查库。
    *   **业务操作锁 (按需)：** 针对多步骤业务操作 (如 `lock:biz_operation:create_order:{userId}`)。

*   **锁超时与获取策略：**
    *   **获取锁超时 (`tryLock`):** 50-100毫秒 (原10ms可能过短)。
    *   **锁持有超时 (Lease Time):** 10-30秒 (需大于缓存重建耗时)。Redisson Watchdog自动续期更优。
    *   **获取锁失败后的行为：**
        *   **逻辑过期热点数据：** **直接返回旧（但可用）缓存数据** (`getFromLogicalCache` 逻辑)。理由：性能与用户体验优先，避免请求阻塞；系统稳定性，防锁竞争致雪崩；秒杀等场景下，展示旧信息影响相对小，关键库存/价格有独立保障。
        *   **物理过期且无旧数据：** 短暂等待 (如 `Semaphore` 限制并发重建) 或快速失败。

#### **1.5 热点数据处理机制 (结合 `TicketHeatManager.java`)**

*   **热点识别：** `recordAccess` 和 `isHotTicket` 动态识别 (如超 `HOT_THRESHOLD`)。
*   **缓存策略：** 热点数据用逻辑过期 (`shouldUseLogicalExpire`) 和更长TTL (`HOT_CACHE_TTL`)。
*   **预热机制：**
    *   **定时预热：** XXL-Job (`cachePrewarmJob`, `prewarmHotData`) 定期加载热点到Redis和Caffeine。`TicketHeatEvaluationTask.java` `preloadNewHotTicket` 体现此逻辑。
    *   **实时预热：** 数据刚变热点时触发即时预热。
*   **缓存重建 (互斥锁 + 异步更新):**
    *   **主线程 (`getFromLogicalCache`):**
        1.  检查Redis数据逻辑过期时间。
        2.  未过期，返回数据。
        3.  已逻辑过期，尝试获取分布式锁 (`lock:cache_rebuild:{businessType}:{id}`)，短超时 (50-100ms)。
        4.  **获取锁成功：** 启动**异步线程** (`CACHE_REBUILD_EXECUTOR` 线程池) 执行DB查询和缓存回填。主线程**仍可先返回旧的、逻辑过期但尚可用的数据**，保低延迟。
        5.  **获取锁失败：** 其他线程直接返回旧的、逻辑过期但尚可用的数据。
    *   **异步线程 (缓存重建任务):**
        1.  DB加载最新数据。
        2.  新数据写入Redis，更新逻辑过期时间。
        3.  通过消息机制 (如Redis Pub/Sub) 通知其他应用实例失效其Caffeine旧缓存。
        4.  释放分布式锁。
        5.  含失败重试 (如最多3次，指数退避)。

#### **1.6 “旧数据”问题考量与影响**

*   **数据时效性要求：**
    *   **价格：** 展示时允许旧缓存价格。**提交订单结算时**，后端必须从DB或强一致源重获最新价格。
    *   **库存：** 展示时允许旧缓存库存。**提交订单时**，必须通过Lua实时检查和预扣减Redis“交易层库存”。支付完成后更新DB“持久层库存”。用户看到的过期库存主要影响浏览，不致下单超卖。
*   **用户体验：**
    *   略微过期的信息通常优于长时间等待或系统错误。
    *   关键在于核心交易流程（下单、支付）的数据准确性。

### **二、 库存管理与防超卖优化**

#### **2.1 库存管理分层设计 (三层库存架构)**

*   **展示层库存 (Redis - HASH/STRING):**
    *   **用途：** 商品列表/详情页库存展示。
    *   **来源：** 持久层库存通过Canal+MQ或定时任务同步，允许秒级延迟。
    *   **特性：** 高读性能，允许短暂不一致，提升浏览体验，减小交易层库存压力。
    *   **更新：** 异步，交易层/持久层库存变化后消息通知更新。

*   **交易层库存 (Redis - STRING/INTEGER, 原子操作):**
    *   **用途：** 下单时库存检查和**预扣减**，防超卖核心。
    *   **来源：** 活动前从持久层初始化。后续扣减/回滚直接在此层原子操作。
    *   **特性：** 必须保证高并发下原子性与准确性。用Redis `INCRBY`/`DECRBY`或Lua (如 `stock_check.lua`, `seckill.lua`)。
    *   **与持久层同步：** 预扣减成功 -> 订单入库。支付成功 -> 触发持久层库存最终扣减。

*   **持久层库存 (MySQL - 商品表/SKU表库存字段):**
    *   **用途：** 库存最终存储、后台管理、数据核对与恢复基准。
    *   **特性：** 强一致性 (ACID)。
    *   **更新：** 活动库存加载到交易层；支付成功后最终扣减；订单取消/退款/补货时更新。

#### **2.2 防超卖核心机制与流程**

**2.2.1 库存检查与扣减时机和方式**

*   **检查时机：**
    1.  **用户点击“立即购买”/提交订单时 (订单创建前):** **首次关键检查**。调用Lua脚本，原子性检查并**预扣减**Redis**交易层库存**。
    2.  **支付完成后 (可选，推荐二次校验):** 更新DB真实库存前，轻量级校验预扣减记录有效性。

*   **扣减方式：**
    1.  **预扣减：** Redis交易层库存，Lua保证原子性。
    2.  **真实扣减：** 用户支付成功，订单状态更新后，异步（但可靠地）扣减MySQL持久层库存。

**2.2.2 原子性保证 (Lua脚本)**

*   **Redis Lua 脚本：** 高并发库存操作原子性核心。
    *   **`stock_check.lua` (或类似) 逻辑：** (1) 收参: SKU ID, 数量。(2) `GET` SKU交易层库存。(3) 若 `nil` 或不足，返失败。(4) 若充足，`DECRBY` 扣减。(5) 返成功。
    *   **优点：** Redis将Lua脚本作单个原子命令执行，无并发竞争。
*   **分布式锁 (辅助)：** 若Lua调用前后有其他需同步的临界区资源（如用户购买资格）。单纯库存扣减，Lua已足够。

#### **2.3 库存回滚机制**

**2.3.1 预扣减与库存占用**

*   Lua预扣减Redis交易层库存后，该部分库存被“占用”或“锁定”，等待支付。若不支付，则占用至订单超时取消。

**2.3.2 订单超时库存回滚 (15分钟未支付)**

*   **触发：** XXL-Job定时任务 (如 `OrderStateAutoTransferTask.java` `autoCancelUnpaidOrdersJob`)。每分钟扫描，分片处理。
*   **回滚流程：**
    1.  查询超时“待支付”订单。
    2.  DB中更新订单状态为“已取消”/“超时关闭”。
    3.  **回滚Redis交易层库存：** 获取SKU ID和数量，调用Lua脚本 (`stock_rollback.lua`) 原子性`INCRBY`加回。
    4.  **回滚持久层库存 (如适用)：** 若持久层库存有“锁定”状态，则释放。通常持久层在支付后才扣，故此步不常需。
    5.  清理相关缓存 (如订单详情)。
    6.  记录日志与监控。

**2.3.3 Redis与MySQL库存同步**

*   **MySQL -> Redis (交易层/展示层 - 初始化/补货):** 全量同步 (系统启动、大补货) 或增量同步 (后台调库存 -> Canal+MQ -> 应用更新Redis)。
*   **Redis交易层 -> MySQL持久层 (正常销售扣减):** 支付成功 -> 可靠事件 (MQ) -> 消费者扣减MySQL真实库存 (幂等、可靠)。
*   **Redis交易层 -> Redis展示层 (非实时同步):** 交易层库存变化后，异步消息更新展示层，允许延迟。

**2.3.4 解决“大量下单不支付占用库存”问题**

*   **标准方案：** 预扣减 + 超时自动回滚。15分钟支付窗口是体验与库存利用的平衡。
*   **风控策略：** 限制单用户短时未支付订单数；识别异常IP/用户行为。
*   **动态调整支付时长：** 紧俏商品可缩短支付时间 (如5分钟)。
*   **用户提示：** 明确告知订单有效支付时间。

### **三、 高并发场景优化策略 (秒杀等)**

#### **3.1 流量削峰与系统保护**

*   **多级限流 (Sentinel):**
    *   **网关/Nginx层：** IP、连接频率初筛。
    *   **应用层 (`SentinelConfig.java`):** QPS限流 (`seckillRule`，启用预热)，线程数限流 (`orderRule`)，系统自适应限流 (基于CPU、最大线程数)。
*   **排队机制：** 超出处理能力的请求入队 (Redis List或MQ)，前端轮询/长连接获取结果。
*   **答题/验证码：** 增加门槛，过滤机器流量。
*   **前端优化：** 动静分离，CDN，按钮防重。

#### **3.2 缓存预热与数据准备**

*   **热点数据预热：** 提前确定秒杀商品，通过XXL-Job (`cachePrewarmJob`) 将商品详情、库存等加载到Caffeine和Redis。
*   **布隆过滤器预加载 (`BloomFilter.java`):** 将有效商品ID (尤其秒杀商品) 加载到布隆过滤器，快速过滤无效请求，防缓存穿透 (`BloomFilterRebuildJob.java`)。

#### **3.3 异步化处理**

*   非核心流程 (日志、统计、通知) 从主交易链路剥离，通过MQ异步处理。

### **四、 监控与运维保障**

*   **缓存监控：** Caffeine/Redis命中率、内存、连接数、慢查询、Key分布。
*   **库存监控：** 各层库存一致性、预扣减成功率、回滚情况。
*   **数据同步链路监控：** Canal解析延迟、MQ消息堆积、消费成功率/延迟。
*   **定时任务监控：** XXL-Job执行状态、成功率、耗时。
*   **全链路压测与应急预案：** 定期压测，验证方案，备好应急预案 (一键降级、限流加码、扩容)。




**接口层防重机制**：在Controller层引入请求唯一标识Token机制，用户发起敏感操作前先获取防重Token并存储在Redis中，请求时携带Token进行验证，验证通过后立即删除Token确保单次使用；同时结合用户ID+业务场景构建复合防重键，设置合理的时间窗口（如订单创建30秒内防重），有效拦截前端重复点击和网络重传导致的重复请求。

**服务层并发控制**：针对核心业务操作如秒杀下单、库存扣减、优惠券核销等场景，使用Redisson分布式锁确保同一资源的操作串行化执行；锁粒度细化到具体资源（如voucher:lock:123、stock:lock:456），避免全局锁影响性能；结合看门狗机制自动续期，防止业务执行时间过长导致锁超时；对于Redis批量操作，使用Lua脚本保证原子性，如库存检查-扣减、优惠券状态验证-更新等复合操作在同一个事务中完成。

**数据层一致性保障**：在数据库设计层面，为订单表、优惠券订单表等核心表添加业务唯一索引（如用户ID+优惠券ID+日期），从根本上杜绝重复数据产生；采用乐观锁机制，在更新库存、优惠券状态时增加版本号或时间戳校验；对于分布式场景下的最终一致性问题，引入本地消息表和补偿机制，确保跨服务的幂等性；同时在缓存层面利用已有的UnifiedCache和BloomFilter，对幂等性验证结果进行缓存，提升重复请求的响应速度并减少数据库压力。

整套方案形成了从前端到数据库的完整防护链路，既保证了用户体验的流畅性，又确保了数据的强一致性，特别适合处理秒杀、支付等高并发敏感业务场景。