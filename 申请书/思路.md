系统从用户配置的模型列表中选择优先级最高的模型发起调用。一旦调用失败，系统会判断失败的具体原因（例如请求超时、速率限制等），并依据错误类型，执行针对性的重试或者降级逻辑。

系统使用唯一的对话ID来管理会话，并将对话内容保存在内存中用于常规记录，同时也会存入向量数据库。

构建API代理，将代理API key和真实API key映射，并可以在网关配置中为每个代理key定制独立的调用配额（如请求数限制或Token用量上限）。API网关在接收请求时，会校验代理API密钥的有效性并执行相应的限流检查。若请求符合配额规定，则使用真实API key调用AI服务。

拦截AI模型调用，为请求内容生成语义向量，然后在向量数据库中搜索与此向量语义最相近的已缓存请求向量。对检索到的候选缓存进行与预设阈值比较，若找到满足条件的语义相似缓存，则返回其响应。


好的，这是一个基于您提供的思路，并进行优化和细化的完整技术方案。

**ShenYu网关集成Spring AI技术方案**

**1. 架构设计目标**

*   **核心替换**: 以Spring AI为核心，替换当前ShenYu AI插件中自定义的AI模型调用和管理逻辑。
*   **架构兼容**: 无缝融入ShenYu现有插件化架构、SPI机制和配置体系，最小化对现有核心流程的侵入。
*   **能力增强**: 显著提升AI能力，利用Spring AI支持更广泛的AI模型提供商和模型类型（如文本、图像、向量等未来扩展）。
*   **统一管理**: 通过Spring AI实现AI模型配置、调用的统一化、标准化，简化运维和二次开发。

**2. 核心模块重构方案**

**2.1 `shenyu-plugin-ai-common` 模块改造 (AI基础与配置)**

*   **当前功能**: 提供AI通用配置(如`AICommonConfig`)和策略接口(如`AiModel`)。
*   **改造方向**:
    1.  **依赖引入**:
        *   引入`spring-ai-core`作为核心依赖。
        *   根据需要引入特定模型的Spring AI Starter，例如 `spring-ai-openai-spring-boot-starter`, `spring-ai-ollama-spring-boot-starter`等。这些Starter通常会带来相应的`ChatClient`实现。
    2.  **`AiModel`接口适配**:
        *   保留或微调现有的`AiModel`接口，使其方法签名能够适配Spring AI的`ChatClient`的核心功能（如`call(Prompt)`和`stream(Prompt)`）。
        *   考虑新增一个`SpringAiModelAdapter`实现该接口，内部封装一个Spring AI的`ChatClient`实例。
    3.  **`AICommonConfig`配置扩展**:
        *   扩展`AICommonConfig`或引入新的配置类，用于承载Spring AI特定模型的配置属性（如API Key, endpoint, model name, temperature, topP等）。
        *   这些配置将从ShenYu的`PluginData`中解析，并用于动态构建和配置Spring AI的`ChatClient`实例。
    4.  **`SpringAiModelFactory` (模型工厂)**:
        *   新增`SpringAiModelFactory`，取代现有的`AiModelFactory`。
        *   此工厂职责：
            *   根据`PluginData`中的配置（包括模型类型，如OpenAI, Ollama等，以及具体连接参数），动态创建和配置相应的Spring AI `ChatClient` (例如 `OpenAiChatClient`, `OllamaChatClient`)。
            *   将创建的`ChatClient`封装到`SpringAiModelAdapter`中，并返回`AiModel`接口实例。
            *   管理已创建的`ChatClient`实例的生命周期和缓存，避免重复创建。

**2.2 AI模型策略层重构 (模型适配与调用)**

*   **替换策略**:
    1.  **移除自定义实现**: 废弃或移除所有针对特定AI服务商（如OpenAI）的自定义HTTP客户端和API调用逻辑。
    2.  **统一适配器**:
        *   主要通过`SpringAiModelAdapter`实现。该适配器接收ShenYu插件层传递的请求参数（如用户prompt、上下文等）。
        *   内部将这些参数转换为Spring AI的`Prompt`对象。
        *   调用其持有的`ChatClient`实例的`call()`或`stream()`方法。
        *   将`ChatResponse`或`Flux<ChatResponse>`转换回ShenYu插件期望的格式。
    3.  **多提供商支持**:
        *   利用Spring AI对多提供商的内置支持。`SpringAiModelFactory`根据配置动态选择并实例化对应的`ChatClient`。
        *   ShenYu的插件配置（`PluginData`）中需要明确指定`modelProvider` (如 `openai`, `azure-openai`, `ollama`, `gemini`等) 和相关的认证、连接信息。

**2.3 `shenyu-plugin-ai-proxy` 模块升级 (核心代理逻辑)**

*   **核心改造**:
    1.  **`AiProxyPlugin`集成**:
        *   `AiProxyPlugin`通过`SpringAiModelFactory`获取配置好的`AiModel`实例（即`SpringAiModelAdapter`）。
        *   将请求体（或经过`shenyu-plugin-ai-prompt`处理后的Prompt）传递给`AiModel`进行调用。
    2.  **Prompt Template支持 (协同)**:
        *   虽然`shenyu-plugin-ai-prompt`主要负责模板处理，`AiProxyPlugin`需能接收处理后的最终Prompt。
        *   如果需要更灵活的模板参数注入，可考虑在`AiProxyPlugin`中，结合`SpringAiModelAdapter`进一步使用Spring AI的`PromptTemplate`功能，将请求中的动态参数填充到模板中。
    3.  **错误处理与重试**:
        *   适配Spring AI可能抛出的特定异常（如`OpenAiHttpException`等），将其转换为统一的ShenYu错误响应。
        *   可以结合ShenYu现有的重试机制，或者利用Spring Retry对`ChatClient`的调用进行封装，以增强服务调用的健壮性。
    4.  **响应格式化与结构化输出**:
        *   利用Spring AI的`OutputParser` (如`BeanOutputParser`, `MapOutputParser`, `ListOutputParser`)。
        *   `SpringAiModelAdapter`或`AiProxyPlugin`可以在调用`ChatClient`后，使用`OutputParser`将模型的文本响应转换为结构化的Java对象或JSON，方便下游处理。配置中应能指定期望的输出格式。

**2.4 `shenyu-plugin-ai-prompt` 模块增强 (提示词工程)**

*   **功能扩展**:
    1.  **`PromptTemplate`引擎集成**:
        *   引入Spring AI的`org.springframework.ai.prompt.PromptTemplate`类。
        *   该模块负责从请求或配置中获取原始用户输入和模板标识。
        *   加载对应的模板内容（可来自配置中心、数据库或本地文件）。
        *   使用`PromptTemplate.render(Map<String, Object> variables)`方法，将用户输入和上下文变量填充到模板中，生成最终的Prompt字符串或`Prompt`对象。
    2.  **动态Prompt模板管理**:
        *   模板本身可以通过ShenYu配置中心进行管理和动态更新。
        *   插件启动时加载模板，并监听配置变更以热更新模板。
    3.  **Prompt工程最佳实践模板库**:
        *   预置一套常用的、高质量的Prompt模板（如角色扮演、摘要生成、代码解释等），作为最佳实践供用户选择和参考。
    4.  **多轮对话上下文管理**:
        *   利用Spring AI的`ChatMemory`接口及其实现（如`InMemoryChatMemory`）或自定义实现。
        *   `shenyu-plugin-ai-prompt`或`AiProxyPlugin`负责：
            *   为每个会话（可基于用户ID、会话ID等）维护一个`ChatMemory`实例。
            *   在构建`Prompt`时，将历史对话消息（从`ChatMemory`中获取）加入到`Prompt`的`Message`列表中。
            *   在收到AI响应后，将用户的新提问和AI的回答更新回`ChatMemory`。
            *   需要考虑`ChatMemory`的存储和过期策略。

**2.5 `shenyu-plugin-ai-token-limiter` 模块优化 (限流与成本控制)**

*   **增强功能**:
    1.  **Token计数集成**:
        *   Spring AI本身不直接提供统一的`TokenCount`接口，但某些`ChatClient`实现（如OpenAI）可能在响应中返回token使用信息。
        *   对于预估，可以引入如`tiktoken` (for OpenAI compatible models) 或其他特定模型的tokenizer库。
        *   `AiTokenLimiterPlugin`需要根据当前使用的AI模型类型，选择合适的Token计算方法。
        *   优先使用模型响应中返回的精确Token数；如果不可用或需要在请求前预估，则使用tokenizer库。
    2.  **模型特定Token计算策略**:
        *   维护一个策略映射，根据模型类型（如`gpt-3.5-turbo`, `claude-3-opus`, `gemini-pro`）选择不同的tokenizer或不同的处理方式。
    3.  **基于模型类型的限流策略**:
        *   允许在ShenYu的限流配置中，针对不同的AI模型提供商或具体模型设置不同的Token限流阈值。
    4.  **成本控制与预算管理 (高级功能)**:
        *   记录每次调用的Token消耗（输入+输出）。
        *   结合预设的模型价格，估算调用成本。
        *   可以与外部监控系统集成，或在ShenYu层面提供简单的累计Token消耗和成本概览。
        *   达到预设预算阈值时，可触发告警或更严格的限流。

**3. 配置与管理**

**3.1 Spring ApplicationContext管理**
*   **关键挑战**: ShenYu插件是动态加载的，而Spring AI通常依赖Spring Boot的自动配置和Spring上下文。
*   **方案**:
    1.  **独立Spring Context**: 为AI插件功能（或每个启用了AI能力的插件实例）创建一个独立的、轻量级的Spring `AnnotationConfigApplicationContext`。
    2.  **编程式配置**: 在插件初始化时，根据ShenYu的`PluginData`动态、编程式地配置Spring AI相关的Bean（如`ChatClient`的各种属性、`ChatMemory`等）。
    3.  **Bean获取**: ShenYu插件通过这个独立的Context获取`SpringAiModelFactory`或直接获取`AiModel`实例。
    4.  **生命周期管理**: ShenYu插件负责管理这个独立Context的生命周期（启动、关闭）。

**3.2 配置体系集成**

*   **`PluginData`扩展**:
    *   在AI相关插件的`PluginData` JSON配置中，增加专门的`springAiConfig`字段。
    *   `springAiConfig`内部结构应能映射到Spring AI的配置属性，例如：
        ```json
        {
          "modelProvider": "openai", // أو "ollama", "azureOpenai"
          "openai": { // 与spring.ai.openai.* 对应
            "apiKey": "sk-...",
            "model": "gpt-4",
            "temperature": 0.7
          },
          "ollama": { // 与spring.ai.ollama.* 对应
             "baseUrl": "http://localhost:11434",
             "chat": {
                 "model": "llama3",
                 "options": { "temperature": 0.8 }
             }
          },
          "outputParser": { // 可选，用于结构化输出
            "format": "bean", // "map", "list"
            "beanClass": "com.example.MyResponseBean"
          },
          "chatMemory": { // 可选，用于多轮对话
            "enabled": true,
            "type": "inMemory", // "redis" 等未来扩展
            "maxTokens": 2000 // 示例：内存中保留的最大token数
          }
          // ... 其他通用配置
        }
        ```
*   **动态更新**:
    *   当ShenYu配置中心的`PluginData`发生变更时，AI插件会收到通知。
    *   插件需要销毁旧的Spring AI `ChatClient`实例（及其相关的`SpringAiModelAdapter`）和可能存在的独立Spring Context，然后根据新配置重新初始化和创建。确保资源正确释放。
*   **兼容性**: 保持与ShenYu配置中心（Nacos, ZooKeeper, Etcd, Apollo等）的现有集成方式不变。

**3.3 模型管理策略 (增强)**

*   **模型注册发现 (逻辑层面)**:
    *   ShenYu本身不直接注册AI模型到外部服务。这里的“注册发现”更多指ShenYu能够感知和管理通过`PluginData`配置的多个AI模型实例。
    *   可以通过Admin后台界面展示当前配置了哪些AI模型及其状态。
*   **模型健康检查**:
    *   `SpringAiModelFactory`或一个专门的健康检查服务可以定期（或按需）对配置的`ChatClient`实例进行简单的“ping”测试（如发送一个非常短的、低成本的prompt）。
    *   健康检查结果可以更新到模型实例的状态中。
*   **故障转移 (可选的高级功能)**:
    *   如果配置了主备AI模型（例如，主OpenAI，备Azure OpenAI），当主模型健康检查失败或连续调用失败时，`SpringAiModelFactory`或`AiProxyPlugin`可以将流量自动切换到备用模型。这需要在`PluginData`中支持定义主备关系和切换策略。
*   **模型性能监控**:
    *   集成ShenYu的监控体系（如Prometheus metrics）。
    *   `SpringAiModelAdapter`在每次调用前后记录关键指标：
        *   调用延迟。
        *   请求和响应的Token数 (如果可用)。
        *   成功/失败次数。
        *   按模型提供商/模型名称维度进行聚合。

**4. 插件执行流程重构**

假设一个典型的AI调用请求流经以下顺序（可根据实际插件链调整）：

1.  **请求进入ShenYu网关**
2.  **`shenyu-plugin-ai-token-limiter` (AI Token 限流插件)**:
    *   解析请求，获取用户标识和即将调用的AI模型类型（从请求头或路径中获取，或从下游插件的配置预知）。
    *   **如果需要预估Token**: 根据模型类型选择合适的tokenizer库，对用户输入（或部分输入，取决于具体限流内容）进行Token化，计算Token数。
    *   **限流判断**: 对比计算出的Token数（或预期Token数）与该用户/该模型类型的限流策略。
    *   **限流处理**: 如果超出限制，则拒绝请求；否则，传递给下一个插件。
3.  **`shenyu-plugin-ai-prompt` (AI Prompt 处理插件)**:
    *   获取用户原始输入、上下文变量（可能来自请求参数、Header或上一个插件）。
    *   根据配置或请求指定，加载Prompt模板。
    *   **上下文管理**: 如果启用了多轮对话，从`ChatMemory`中检索历史对话。
    *   使用Spring AI的`PromptTemplate`引擎，将用户输入、变量和历史对话填充到模板中，生成最终的`Prompt`对象或格式化后的Prompt字符串。
    *   将处理后的Prompt传递给下一个插件。
4.  **`shenyu-plugin-ai-proxy` (AI 代理插件)**:
    *   从`SpringAiModelFactory`获取对应配置的`AiModel`实例（内部为`SpringAiModelAdapter`，持有`ChatClient`）。
    *   接收来自`shenyu-plugin-ai-prompt`处理后的`Prompt`对象（或字符串，此时需再包装成`Prompt`对象）。
    *   调用`AiModel`的`call(Prompt)`或`stream(Prompt)`方法，实际委托给Spring AI的`ChatClient`与AI服务商进行通信。
    *   **响应处理**:
        *   获取`ChatResponse`。
        *   如果配置了`OutputParser`，则使用它将AI的文本响应转换为结构化数据。
        *   **Token数更新 (后置)**: 如果AI服务商在响应中返回了精确的Token使用数，`AiTokenLimiterPlugin`（如果设计为可双向交互或通过共享上下文）或`AiProxyPlugin`本身可以记录此信息用于更精确的统计或后续限流。
        *   **上下文更新**: 如果启用了多轮对话，将当前的用户提问和AI的回答更新到`ChatMemory`中。
    *   将最终结果（结构化数据或原始文本）返回给调用方。
5.  **响应返回给客户端**

**5. 关键考量点**

*   **异步与响应式**: ShenYu基于WebFlux，是响应式的。Spring AI的`ChatClient`也支持返回`Flux<ChatResponse>`用于流式处理。在整个集成过程中，应尽可能保持响应式链路，避免阻塞操作。
*   **安全性**: API密钥等敏感信息必须安全存储和管理，可利用ShenYu现有的加密/脱敏机制，或集成Vault等外部密钥管理系统。
*   **可测试性**: 每个模块的改造都需要充分的单元测试和集成测试。特别是`SpringAiModelFactory`和`SpringAiModelAdapter`的逻辑。
*   **版本兼容性**: 注意Spring AI版本与所选AI模型提供商SDK的兼容性。

这个方案旨在充分利用Spring AI的强大功能和生态，同时保持对ShenYu现有体系的尊重和兼容，从而实现一个现代化、可扩展、易于维护的AI网关能力。



**ShenYu AI调用降级与重试技术方案**

**1. 整体目标与范围**

本方案旨在为ShenYu网关的AI插件（特别是`shenyu-plugin-ai-proxy`或一个新增的AI编排插件）设计并实现一个健壮、灵活且智能的AI模型调用降级与重试机制。目标是：

*   **提升服务可靠性**: 即使主要AI模型不可用或性能下降，也能通过自动降级保障AI功能的持续可用。
*   **优化成本与性能**: 优先使用配置的高优先级、高性价比模型，在满足需求的前提下，智能选择备用模型。
*   **增强灵活性**: 支持多种AI提供商和模型，允许通过配置动态调整降级和重试策略。
*   **提升可观测性**: 提供详细的日志、监控指标和告警，便于追踪问题、分析性能和优化策略。

**2. 核心架构组件 (基于ShenYu SPI 和 Spring AI)**

我们将引入或增强以下组件，大部分可以设计为ShenYu的SPI接口，以便扩展：

**2.1. `AIModelRegistryService` (AI模型注册表服务 - SPI)**

*   **职责**: 集中管理所有已配置的、可供ShenYu AI插件使用的AI模型信息。
*   **数据来源**: 从ShenYu的`PluginData`（特定于AI编排/代理插件或全局AI配置）中加载和动态更新。
*   **模型信息结构 
    *   `modelId`: 模型的唯一标识符 (e.g., "shenyu-openai-gpt4-turbo", "shenyu-anthropic-claude3-sonnet")。此ID将在ShenYu内部使用，并映射到Spring AI配置中相应的模型名称。
    *   `provider`: 模型提供商 (e.g., "openai", "azureOpenai", "anthropic", "googleAiGemini", "ollama")。这对应Spring AI的提供商标识。
    *   `springAiBeanNamePrefix`: (新增) 用于在Spring AI上下文中定位此模型`ChatClient`等相关bean的前缀或唯一标识。
    *   `priority`: 整数，调用优先级（1为最高）。
    *   `costTier`: 成本级别 ("high", "medium", "low") 或具体参考价格。
    *   `capabilities`: 列表 (e.g., "GENERAL_QA", "CODE_GENERATION", "SUMMARIZATION", "TOOL_CALLING_V1", "IMAGE_INPUT", "STREAMING").
    *   `rateLimits`: (可选) 配置的该模型账户速率限制信息 (TPM, RPM)。
    *   `contextWindow`: Token上下文窗口大小。
    *   `status`: (动态) 模型当前健康状态 (e.g., "HEALTHY", "DEGRADED", "UNAVAILABLE", "CIRCUIT_OPEN")。
    *   `healthCheckEndpoint`: (可选) 用于主动健康检查的简单测试prompt或API。
    *   `metadata`: 其他自定义标签。
*   **功能**:
    *   提供按优先级、能力、状态等条件查询可用模型的方法。
    *   支持配置动态更新。

**2.2. `AIErrorClassifier` (AI错误分类器 - SPI)**

*   **职责**: 将来自不同Spring AI `ChatClient` (及其底层SDK) 的原始异常/错误码，映射为系统内部定义的标准错误类型。
*   **标准错误类型 (示例)**:
    *   `RATE_LIMIT_ERROR` (HTTP 429)
    *   `AUTHENTICATION_ERROR` (HTTP 401, 403)
    *   `SERVER_ERROR` (HTTP 5xx)
    *   `TIMEOUT_ERROR` (ConnectTimeout, ReadTimeout)
    *   `CAPABILITY_MISMATCH_ERROR` (模型不支持请求的功能)
    *   `CONTENT_POLICY_ERROR` (内容审查拒绝)
    *   `INVALID_REQUEST_ERROR` (HTTP 400, 请求格式或参数问题)
    *   `MODEL_UNAVAILABLE_ERROR` (模型当前不可用或不存在)
    *   `UNKNOWN_ERROR`
*   **实现**: 为每个支持的Spring AI提供商（如OpenAI, Azure, Ollama）提供一个具体的分类器实现，处理其特有的异常类型和错误信息。

**2.3. `FallbackRetryPolicy` (降级与重试策略)**

*   **定义**: 这不是一个单一的服务，而是一套配置规则，通常与`AIModelRegistryService`中的模型条目或全局AI插件配置相关联。
*   **配置内容 (在`PluginData`中定义)**:
    *   **全局策略**: 默认的重试和降级行为。
    *   **模型特定策略**: 针对特定`modelId`或`provider`覆盖全局策略。
    *   **Retry Policy (针对当前模型)**:
        *   `enabled`: `true` | `false`.
        *   `maxAttempts`: 最大重试次数。
        *   `backoffStrategy`: `FIXED`, `EXPONENTIAL`.
        *   `initialIntervalMs`, `maxIntervalMs`, `multiplier`.
        *   `retryableStandardErrorTypes`: 可重试的标准错误类型列表 (e.g., `[RATE_LIMIT_ERROR, SERVER_ERROR, TIMEOUT_ERROR]`).
        *   `nonRetryableStandardErrorTypes`: 不可重试的错误类型列表 (e.g., `[AUTHENTICATION_ERROR, INVALID_REQUEST_ERROR, CONTENT_POLICY_ERROR]`).
    *   **Fallback Policy (切换到其他模型)**:
        *   `triggerConditions`: 触发降级的标准错误类型列表 (e.g., `[SERVER_ERROR, MODEL_UNAVAILABLE_ERROR, RATE_LIMIT_ERROR_AFTER_RETRY]`).
        *   `fallbackBehavior`:
            *   `NEXT_AVAILABLE_BY_PRIORITY`: 尝试下一个优先级且健康、能力匹配的模型。
            *   `SPECIFIC_MODEL_LIST`: 尝试指定的模型ID列表（按顺序）。
            *   `FAIL_FAST`: 不进行降级，直接返回错误。
        *   `maxFallbackAttempts`: 最大降级尝试次数（避免无限循环）。

**2.4. `AICircuitBreakerService` (AI熔断器服务 - SPI)**

*   **职责**: 为每个`(modelId, provider)`组合维护一个熔断器状态，防止向已知有问题的模型持续发送请求。
*   **实现**: 可以基于成熟的库（如Resilience4j）或自定义实现。
*   **状态**: `CLOSED`, `OPEN`, `HALF_OPEN`.
*   **参数 (可配置)**: 失败率阈值、慢调用率阈值、等待时间（OPEN状态持续时间）、半开状态允许的请求数。
*   **集成**: 在每次模型调用前检查状态，调用失败后根据错误类型更新熔断器统计。

**2.5. `AIProviderAdapterFactory` (扩展自 `SpringAiModelFactory`)**

*   **职责**:
    *   基于`AIModelRegistryService`中的模型配置（特别是`springAiBeanNamePrefix`或完整配置），从ShenYu管理的Spring AI上下文中获取或动态创建并配置Spring AI `ChatClient`实例。
    *   封装了与特定提供商交互的逻辑，包括将ShenYu的通用AI请求转换为Spring AI `Prompt`对象。
    *   调用`ChatClient`并处理其响应和异常。

**3. 配置体系 (ShenYu `PluginData`)**

AI插件（如`shenyu-plugin-ai-proxy`或新的AI编排插件）的`PluginData`将扩展以支持以上所有配置：

```json
// Example PluginData structure
{
  "aiModelRegistry": [
    // ... 模型条目，如用户笔记中所示，增加 springAiBeanNamePrefix, status, healthCheckEndpoint
    {
      "modelId": "openai-gpt4", "provider": "openai", "springAiBeanNamePrefix": "openaiGpt4",
      "priority": 1, "costTier": "high", "capabilities": ["GENERAL_QA", "TOOL_CALLING_V2"],
      "contextWindow": 128000, "status": "HEALTHY", /* ... */
      "retryPolicyRef": "defaultRetry", "fallbackPolicyRef": "defaultFallback"
    },
    {
      "modelId": "ollama-llama3", "provider": "ollama", "springAiBeanNamePrefix": "ollamaLlama3",
      "priority": 2, "costTier": "low", "capabilities": ["GENERAL_QA"],
      "contextWindow": 8000, "status": "HEALTHY", /* ... */
      "retryPolicyRef": "ollamaRetry", "fallbackPolicyRef": "defaultFallback"
    }
  ],
  "retryPolicies": {
    "defaultRetry": { "enabled": true, "maxAttempts": 3, "backoffStrategy": "EXPONENTIAL", /* ... */ },
    "ollamaRetry": { "enabled": true, "maxAttempts": 2, "retryableStandardErrorTypes": ["TIMEOUT_ERROR"] }
  },
  "fallbackPolicies": {
    "defaultFallback": {
      "triggerConditions": ["SERVER_ERROR", "MODEL_UNAVAILABLE_ERROR"],
      "fallbackBehavior": "NEXT_AVAILABLE_BY_PRIORITY",
      "maxFallbackAttempts": 3
    }
  },
  "circuitBreakerConfig": {
    "default": { "failureRateThreshold": 50, "waitDurationInOpenStateMs": 30000 /* ... */ },
    "openai-gpt4": { /* specific overrides */ }
  },
  "errorClassifierMappings": { // 可选，用于更细致的自定义错误映射
    "openai": { "400_USER_LIMIT_REACHED": "RATE_LIMIT_ERROR" }
  }
}
```

**4. 核心降级与重试执行流程 (在AI代理/编排插件中实现)**

1.  **请求接收与初始模型选择**:
    *   接收上游插件（如`ai-prompt`）处理后的AI请求（包含最终Prompt、期望能力等）。
    *   从`AIModelRegistryService`获取按`priority`排序的可用模型列表。

2.  **迭代调用尝试 (主循环)**:
    *   `currentAttempt = 0`, `fallbackAttempt = 0`
    *   `attemptedModels = new Set()`
    *   **For each `modelInfo` in (filtered and sorted model list from registry)**:
        *   If `modelInfo.modelId` in `attemptedModels`, continue (防止因策略配置不当导致的循环)。
        *   Add `modelInfo.modelId` to `attemptedModels`.
        *   **a. Pre-flight Checks**:
            *   **Capability Check**: 确保`modelInfo.capabilities`满足请求所需。若不满足，记录并跳过此模型。
            *   **Context Window Check**: 检查`modelInfo.contextWindow`是否足够。若不足，根据策略跳过或尝试截断（高级）。
            *   **Circuit Breaker Check**: 调用`AICircuitBreakerService.isAllowed(modelInfo)`。若熔断器打开，记录并跳过此模型。
            *   **Status Check**: 检查`modelInfo.status`是否为不利于调用的状态。
        *   **b. Get `ChatClient`**:
            *   通过`AIProviderAdapterFactory`获取对应`modelInfo`的Spring AI `ChatClient`实例。
        *   **c. Retry Loop (for current model)**:
            *   `retryCount = 0`
            *   Load `retryPolicy` for `modelInfo`.
            *   **Do**:
                *   Try:
                    *   记录尝试调用 (`modelInfo.modelId`, `retryCount`).
                    *   `ChatResponse response = chatClient.call(promptObject);` (或 `stream()`)
                    *   记录成功调用。
                    *   `AICircuitBreakerService.onSuccess(modelInfo)`.
                    *   Return `response` to client (流程结束).
                *   Catch (Exception `e`):
                    *   `standardError = AIErrorClassifier.classify(e, modelInfo.provider)`.
                    *   记录失败调用 (`modelInfo.modelId`, `retryCount`, `standardError`, `e.getMessage()`).
                    *   `AICircuitBreakerService.onError(modelInfo, standardError)`.
                    *   `retryCount++`.
                    *   If `retryPolicy.enabled` AND `retryCount <= retryPolicy.maxAttempts` AND `standardError` in `retryPolicy.retryableStandardErrorTypes`:
                        *   Apply backoff delay.
                        *   Continue retry loop.
                    *   Else (cannot retry on current model):
                        *   Break retry loop, proceed to fallback decision with `standardError`.
        *   **d. Fallback Decision**:
            *   Load `fallbackPolicy` for `modelInfo` or global.
            *   `fallbackAttempt++`.
            *   If `fallbackAttempt > fallbackPolicy.maxFallbackAttempts` OR `standardError` not in `fallbackPolicy.triggerConditions` OR `fallbackPolicy.fallbackBehavior == FAIL_FAST`:
                *   记录最终失败 (所有模型尝试完毕或策略指示不降级)。
                *   Throw final exception / return error response (流程结束).
            *   Else (prepare for next model in outer loop):
                *   记录降级事件 (from `modelInfo.modelId` due to `standardError`).
                *   Continue outer loop to try next model.

3.  **最终失败处理**:
    *   若所有模型尝试完毕仍失败，返回统一的错误响应，包含Trace ID和关键错误信息。

**5. 关键实现细节与考量**

*   **Spring AI `ChatClient`生命周期**: `AIProviderAdapterFactory`需妥善管理`ChatClient`实例。若ShenYu插件为每个请求创建独立的Spring上下文，则`ChatClient`随上下文销毁。若共享上下文，则`ChatClient`可为单例。
*   **动态配置更新**: 所有服务 (`AIModelRegistryService`, `AICircuitBreakerService`等) 需监听ShenYu配置变更事件，热加载新配置（如模型列表、策略参数），并可能需要重置熔断器状态或清理缓存的`ChatClient`。
*   **上下文传递**: 请求ID、用户标识等应在整个调用链中传递，用于日志和监控。
*   **Tool Calling 适配**: 如果降级到不同提供商的模型，其工具调用/Function Calling的定义和实现方式可能不同。`AIProviderAdapterFactory`或一个专门的`ToolCallAdapter`可能需要处理这种异构性，但这非常复杂，初期可简化为仅降级到支持类似工具调用规范的模型。
*   **流式响应 (Streaming)**: 降级和重试逻辑需要兼容Spring AI的流式响应 (`Flux<ChatResponse>`)。错误通常在`Flux`的`onError`信号中捕获。重试流可能意味着重新订阅。
*   **性能开销**: 频繁的配置读取、状态检查、对象创建都可能引入性能开销。需要关注热点路径的优化和缓存策略。

**6. 监控、日志与告警 (集成ShenYu现有体系)**

*   **日志**:
    *   详细记录每个请求的完整调用链：选择的模型、重试次数、遇到的错误 (原始和标准)、降级路径、最终结果。
    *   使用结构化日志，包含Trace ID, Span ID, Model ID, Provider, Standard Error Type。
*   **监控指标 (Metrics - e.g., Prometheus)**:
    *   `shenyu_ai_call_total{model_id, provider, status_code, standard_error_type}`: 调用总数。
    *   `shenyu_ai_call_duration_seconds{model_id, provider}`: 调用延迟。
    *   `shenyu_ai_retry_total{model_id, provider, standard_error_type}`: 重试次数。
    *   `shenyu_ai_fallback_total{from_model_id, to_model_id, trigger_error_type}`: 降级次数。
    *   `shenyu_ai_circuitbreaker_status{model_id, provider, status}`: 熔断器状态变化。
    *   `shenyu_ai_token_usage_total{model_id, provider, type="input|output"}`: Token消耗 (如果Spring AI响应中包含)。
*   **告警**:
    *   高优先级模型持续失败率/错误率超阈值。
    *   降级频率异常增高。
    *   特定模型熔断器长时间处于OPEN状态。
    *   整体AI服务错误率或P99延迟超阈值。

**7. 集成与插件交互**

*   `shenyu-plugin-ai-common`: 提供上述SPI接口定义、标准错误类型枚举、基础数据结构。
*   `shenyu-plugin-ai-proxy` (或新的 `shenyu-plugin-ai-orchestrator`):
    *   实现核心的降级与重试执行流程。
    *   依赖 `AIModelRegistryService`, `AIErrorClassifier`, `AICircuitBreakerService`, `AIProviderAdapterFactory`.
    *   处理来自`shenyu-plugin-ai-prompt`的请求。
*   `shenyu-plugin-ai-token-limiter`: 其信息（如速率限制）可以被`AIModelRegistryService`用作模型选择的一个参考因素，或在错误分类时辅助判断`RATE_LIMIT_ERROR`。

好的，这是一个结合您提供的思路和笔记，并针对ShenYu网关集成Spring AI场景进行优化的完整会话历史管理技术方案。

**ShenYu AI会话历史管理技术方案**

**核心设计原则：**

1.  **抽象化与可插拔 (ShenYu SPI)**: 设计统一的会话历史服务接口，具体的存储实现（内存、Redis、PGVector等）作为可插拔的ShenYu SPI插件。
2.  **Spring AI集成**: 与Spring AI的`ChatMemory`体系兼容或提供适配，方便在AI调用时直接使用。
3.  **高性能与可扩展性**: 优化高频操作（如添加消息、获取近期消息），并考虑大规模并发会话场景。
4.  **功能丰富与健壮性**: 提供必要的会话管理功能（创建、检索、更新、清理、超时），并考虑数据一致性、容错和异步处理。
5.  **双重存储**: 内存用于快速访问和常规记录，PostgreSQL + pgvector用于持久化、语义搜索和高级分析。

**一、核心组件与数据结构 (定义在 `shenyu-plugin-ai-common` 或新的 `shenyu-plugin-ai-memory`)**

1.  **`ConversationId` (会话标识)**:
    *   类型: `String` (通常为UUID)。
    *   生成: 由ShenYu AI插件在会话开始时生成，或从请求中传入。
    *   确保全局唯一性。

2.  **`ChatMessage` (消息结构 - 适配Spring AI `Message<T>`)**:
    *   `id`: `String` (UUID, 消息唯一标识)。
    *   `conversationId`: `String` (关联的会话ID)。
    *   `role`: `org.springframework.ai.chat.messages.MessageType` 枚举 (`USER`, `ASSISTANT`, `SYSTEM`, `FUNCTION`, `TOOL`).
    *   `content`: `String` (消息文本内容)。
    *   `metadata`: `Map<String, Object>` (Spring AI `Message`的`headers`可以映射到这里，存储如`timestamp`, `tokenCount`, `toolCallId`, `toolCallResult`等)。
        *   `timestamp`: `long` (Unix毫秒时间戳)。
        *   (可选) `embeddingVector`: `float[]` (消息内容的向量表示, 用于PGVector)。

3.  **`ChatMemoryStore` (会话历史存储SPI接口 - 适配Spring AI `ChatMemory`)**:
    *   **核心方法 (兼容 `org.springframework.ai.chat.memory.ChatMemory`)**:
        *   `void add(String conversationId, List<org.springframework.ai.chat.messages.Message<?>> messages)`: 向指定会话添加多条消息。
        *   `List<org.springframework.ai.chat.messages.Message<?>> get(String conversationId, int lastN)`: 获取指定会话的最后N条消息。
        *   `void clear(String conversationId)`: 清空指定会话的所有消息。
    *   **扩展方法 (ShenYu特定)**:
        *   `void create(String conversationId, Map<String, Object> metadata)`: 显式创建会话（可选，通常隐式创建）。
        *   `void delete(String conversationId)`: 删除整个会话及其历史记录。
        *   `Optional<Map<String, Object>> getConversationMetadata(String conversationId)`: 获取会话元数据。
        *   `void updateConversationMetadata(String conversationId, Map<String, Object> metadata)`: 更新会话元数据。
        *   `List<ChatMessage> getMessagesByTimeRange(String conversationId, long startTime, long endTime)`: 按时间范围获取消息。
        *   `(高级) List<ChatMessage> getMessagesBySemanticSimilarity(String conversationId, String queryText, int topK, Map<String, Object> filterCriteria)`: 基于语义相似度从PGVector检索消息。

**二、具体存储后端实现 (ShenYu SPI 实现)**

**2.1. `InMemoryChatMemoryStore` (内存存储)**

*   **数据结构**: `ConcurrentHashMap<String, LinkedList<ChatMessage>>` 存储消息，`ConcurrentHashMap<String, Map<String, Object>>` 存储元数据。`LinkedList` 便于按顺序添加和限制大小。
*   **消息存储**: `ChatMessage` 对象直接存储。
*   **特点**: 访问极快，实现简单。数据易失。
*   **生命周期管理**:
    *   **LRU策略**: 可配置最大会话数或最大内存占用。当达到阈值时，淘汰最久未使用的会话（基于元数据中的`lastAccessTime`）。
    *   **超时清理**: 后台线程定期检查会话的`lastAccessTime`，清理超过配置空闲时长的会话。
*   **用途**: 主要用于高频的近期消息访问，为LLM构建上下文。

**2.2. `RedisChatMemoryStore` (Redis存储 - 可选的中间持久层)**

*   **数据结构**:
    *   **消息列表**: Redis List (key: `chatmem:msg:<conversationId>`). 存储 `ChatMessage` 的JSON序列化字符串。`LPUSH`新消息，`LRANGE`获取，`LTRIM`限制长度。
    *   **元数据**: Redis Hash (key: `chatmem:meta:<conversationId>`). 存储会话元数据。
*   **TTL**: 为每个key设置滚动TTL，实现自动过期。
*   **特点**: 持久化，高性能，支持分布式。
*   **用途**: 如果需要比内存更持久但又比PGVector访问更快的中间层，或者作为内存存储的备份/扩展。

**2.3. `PgVectorChatMemoryStore` (PostgreSQL + pgvector 存储)**

*   **职责**: 主要负责消息的持久化存储和语义检索。
*   **数据库表结构**:
    *   **`conversations` 表**:
        *   `id` (UUID, PK, conversation_id)
        *   `user_id` (String, optional, FK to user table)
        *   `created_at` (TimestampTZ)
        *   `last_updated_at` (TimestampTZ)
        *   `metadata` (JSONB)
    *   **`chat_messages` 表**:
        *   `id` (UUID, PK, message_id)
        *   `conversation_id` (UUID, FK to `conversations.id`, Indexed)
        *   `role` (String, e.g., "USER", "ASSISTANT")
        *   `content` (TEXT)
        *   `embedding` (VECTOR(dim), Nullable, Indexed with HNSW/IVFFlat) - 存储消息内容的向量
        *   `timestamp` (TimestampTZ, Indexed)
        *   `metadata` (JSONB)
*   **操作**:
    *   **`add`**:
        1.  将`ChatMessage`对象（不含向量）写入`chat_messages`表。
        2.  更新`conversations`表的`last_updated_at`。
        3.  **异步**: 将`content`发送到嵌入模型服务（如Spring AI `EmbeddingClient`）获取向量，然后更新`chat_messages`表中对应消息的`embedding`字段。
    *   **`get(lastN)`**: 从`chat_messages`按`conversation_id`和`timestamp`倒序查询。
    *   **`getMessagesBySemanticSimilarity`**: 使用`queryText`生成查询向量，然后在`chat_messages`表中针对特定`conversation_id`（或全局）执行向量相似度查询 (e.g., `ORDER BY embedding <-> query_vector LIMIT topK`)。
    *   **`clear`, `delete`**: 执行相应的SQL删除操作。
*   **嵌入模型管理**:
    *   依赖Spring AI的`EmbeddingClient` (如`OpenAiEmbeddingClient`, `OllamaEmbeddingClient`)。
    *   配置中指定使用的嵌入模型。
*   **索引**: 必须为`embedding`列创建合适的向量索引 (如HNSW) 以加速相似度搜索。

**三、会话管理与执行流程 (在 `shenyu-plugin-ai-proxy` 或相关AI插件中)**

1.  **会话初始化/获取**:
    *   从请求中获取或生成`conversationId`。
    *   插件尝试从`ChatMemoryStore` SPI中获取一个实现（可能是组合实现）。

2.  **`ChatMemory` 实例构建 (适配Spring AI)**:
    *   ShenYu AI插件可以提供一个`ChatMemory`的SPI实现，该实现内部委托给配置的`ChatMemoryStore`实例。
    *   这个`ChatMemory`实例可以直接传递给Spring AI的`ChatClient`调用（如果`ChatClient`支持自定义`ChatMemory`）或在调用前手动填充`Prompt`的`Message`列表。

3.  **消息处理流程 (双写策略)**:
    *   **用户请求 -> AI**:
        1.  构建用户`ChatMessage`对象。
        2.  **写入内存存储**: 调用`InMemoryChatMemoryStore.add()`.
        3.  **异步写入PGVector**: 将`ChatMessage`发送到异步队列/线程池，由`PgVectorChatMemoryStore.add()`处理（包括异步获取嵌入向量）。
        4.  从`InMemoryChatMemoryStore.get(lastN)`获取近期对话历史。
        5.  构建Spring AI `Prompt`，调用`ChatClient`。
    *   **AI响应 -> 用户**:
        1.  构建AI助手`ChatMessage`对象。
        2.  **写入内存存储**: 调用`InMemoryChatMemoryStore.add()`.
        3.  **异步写入PGVector**: 同上。
        4.  将AI响应返回给用户。

4.  **构建Prompt上下文**:
    *   **默认**: 从`InMemoryChatMemoryStore`获取最后N条消息。
    *   **RAG场景**:
        1.  获取用户当前问题。
        2.  调用`PgVectorChatMemoryStore.getMessagesBySemanticSimilarity()`检索相关历史片段。
        3.  结合内存中的近期消息和检索到的片段，一起构建Prompt上下文。需要注意Token限制和去重。

5.  **会话生命周期管理**:
    *   **`InMemoryChatMemoryStore`**: 由其内部的LRU和超时清理机制管理。
    *   **`PgVectorChatMemoryStore`**: 数据持久存在，除非显式删除或通过定期归档策略清理。可配置基于`last_updated_at`的归档/删除任务。
    *   **元数据同步**: 会话元数据（如用户ID、标签）应同时更新到内存和PG（如果PG中也存储了会话级元数据）。

**四、高级功能与优化**

1.  **会话历史大小限制 (内存)**:
    *   `InMemoryChatMemoryStore`在`add`时检查消息列表大小，超出则移除最早的。

2.  **会话摘要/压缩 (高级，可选)**:
    *   可设计一个后台任务或在特定条件下触发。
    *   从PGVector中获取较早的消息，使用LLM生成摘要。
    *   将摘要作为一个`SYSTEM`角色的`ChatMessage`存回内存和PGVector，并可能标记或移除被摘要的原始消息。

3.  **异步处理与队列**:
    *   PGVector的写入（尤其是向量生成和DB插入）应完全异步，避免阻塞主调用链路。使用如`ExecutorService`或消息队列 (RabbitMQ, Kafka)。
    *   提供回调或Future来处理写入PGVector的成功/失败。

4.  **数据一致性与容错**:
    *   **内存与PGVector同步**: 双写可能存在短暂不一致。以PGVector为准（Source of Truth for persisted data）。内存作为缓存。
    *   **PGVector写入失败**: 实现重试机制。若持续失败，记录错误，可能需要手动恢复或告警。内存中的数据仍然可用，但持久化失败。
    *   **嵌入失败**: 如果嵌入模型服务失败，消息仍可存入PGVector（不带向量），但无法用于语义搜索。应有重试和告警。

5.  **配置与SPI选择**:
    *   ShenYu插件配置中允许用户选择`ChatMemoryStore`的实现 (e.g., "memory", "pgvector", "composite")。
    *   `CompositeChatMemoryStore`: 可以实现一个组合模式的Store，将写操作分发到多个后端（如内存和PG），读操作根据策略从一个或多个后端聚合。
    *   PGVector连接参数、嵌入模型选择、内存缓存大小、超时等均可配置。

**五、与Spring AI `ChatMemory` 的关系**

*   **选项1: 自定义Spring AI `ChatMemory` 实现**:
    *   创建一个`ShenYuSpringAiChatMemory`类实现`org.springframework.ai.chat.memory.ChatMemory`接口。
    *   该类内部通过ShenYu SPI机制获取配置的`ChatMemoryStore`实例，并将操作委托给它。
    *   这样，ShenYu管理的会话历史可以直接被Spring AI的 `ChatClient` 等组件使用。
*   **选项2: 手动管理**:
    *   ShenYu插件直接使用`ChatMemoryStore`的SPI接口。
    *   在调用Spring AI `ChatClient.call(Prompt)`之前，手动从`ChatMemoryStore`获取消息列表，构建`Prompt`对象。
    *   在收到`ChatResponse`后，手动将AI的回复添加到`ChatMemoryStore`。
    *   这种方式更灵活，但与Spring AI的`ChatMemory`体系解耦。

**推荐方案：** 优先考虑 **选项1**，因为它能更好地融入Spring AI生态，减少重复造轮子，并利用Spring AI可能提供的围绕`ChatMemory`的附加功能。

**六、技术栈与依赖**

*   **ShenYu**: SPI, Plugin system.
*   **Spring AI**: `spring-ai-core`, `spring-ai-[embedding-model-starter]`, `spring-ai-[chat-model-starter]`.
*   **PostgreSQL + pgvector**: 数据库和向量扩展。
Spring Data JPA**: 与PG交互。
*   **Jedis/Lettuce/Spring Data Redis**: (可选) 与Redis交互。
*   **Jackson/Gson**: JSON序列化。
*   **ExecutorService/MQ**: 异步处理。

**实现步骤概览:**

1.  **定义SPI和数据结构**: `ChatMemoryStore`, `ChatMessage`等。
2.  **实现`InMemoryChatMemoryStore`**: 包含LRU和超时。
3.  **实现`PgVectorChatMemoryStore`**:
4.  **(可选) 实现`RedisChatMemoryStore`**.
5.  **实现`ShenYuSpringAiChatMemory`适配器**.
6.  **在AI插件中集成会话管理逻辑**: 双写、Prompt构建、RAG流程。
7.  **配置管理**: 添加相关配置项到ShenYu `PluginData`。
8.  **测试**: 单元测试、集成测试（包括DB和嵌入模型交互）。
9.  **监控与日志**: 集成ShenYu监控体系。

好的，这是一个结合您提供的笔记和需求，针对ShenYu网关实现的完整代理API Key及调用限额技术方案。

**ShenYu网关代理API Key与调用限额技术方案**

**1. 核心目标**

在ShenYu网关层面，为下游的AI服务（通过Spring AI或直接HTTP调用）提供一个代理API Key管理和调用限额控制机制。核心目标包括：

*   **安全隔离**: 保护真实的AI服务API Key，不直接暴露给客户端。
*   **精细化控制**: 为每个生成的代理API Key配置独立的调用配额（请求数、Token用量）。
*   **多维度限流**: 支持多种时间窗口（秒、分、时、天）的限流规则。
*   **动态配置**: 支持通过ShenYu Admin动态管理代理Key、真实Key映射及限流规则。
*   **高性能**: 限流检查和计数更新需高效，不显著影响网关性能。
*   **可观测性**: 提供代理Key使用情况的监控和日志。

**2. 核心组件与实体定义 (基于ShenYu现有体系)**

**2.1. `ProxyApiKey` 实体 (在ShenYu Admin和插件配置中定义)**

*   `proxyKey`: `String` (系统生成或用户指定的代理API Key，全局唯一，具备足够的随机性和长度)。
*   `realKeyName`: `String` (指向一个在 `RealApiKey` 库中注册的真实Key的名称/标识符)。
*   `description`: `String` (可选，描述信息)。
*   `status`: `String` (e.g., "ENABLED", "DISABLED", "EXPIRED").
*   `rateLimitRules`: `List<RateLimitRule>` (关联的限流规则列表)。
*   `metadata`: `Map<String, Object>` (可选，其他自定义信息，如所属租户、应用等)。
*   `createdAt`: `Timestamp`.
*   `updatedAt`: `Timestamp`.
*   `expiresAt`: `Timestamp` (可选，代理Key的过期时间)。

**2.2. `RealApiKey` 实体 (在ShenYu Admin中安全存储)**

*   `keyName`: `String` (真实Key的唯一标识名称，e.g., "openai-prod-key1", "azure-ai-default").
*   `keyValue`: `String` (真实的API Key值，**必须加密存储**)。
*   `provider`: `String` (AI服务提供商，e.g., "OPENAI", "AZURE_OPENAI", "ANTHROPIC", "OLLAMA_INTERNAL").
*   `description`: `String` (可选)。
*   `status`: `String` (e.g., "ACTIVE", "REVOKED").
*   `metadata`: `Map<String, Object>` (可选，如该Key的整体QPS限制等参考信息)。

**2.3. `RateLimitRule` 实体 (作为`ProxyApiKey`的子配置)**

*   `id`: `String` (规则的唯一ID)。
*   `type`: `String` (限流类型: "REQUEST_COUNT", "TOKEN_COUNT").
*   `limit`: `long` (配额上限值)。
*   `timeWindowSeconds`: `long` (时间窗口大小，单位秒，e.g., 60 for 1 minute, 3600 for 1 hour).
*   `algorithm`: `String` (限流算法: "FIXED_WINDOW", "SLIDING_WINDOW_COUNTER" - 推荐).
*   `description`: `String` (可选)。

**2.4. 配置来源 (Key Source Identification)**

*   在ShenYu的插件配置（如`shenyu-plugin-ai-auth`或一个通用的认证插件）中定义如何从请求中提取代理API Key。
*   **支持**:
    *   HTTP Header (e.g., `X-Proxy-API-Key`, `Authorization: Bearer <proxy_key>`).
    *   Query Parameter.
*   配置应允许指定Header名称或参数名称。

**3. 系统架构与ShenYu插件**

**3.1. ShenYu Admin扩展**

*   **真实API Key管理页面**:
    *   增删改查`RealApiKey`实体。
*   **代理API Key管理页面**:
    *   增删改查`ProxyApiKey`实体。
    *   创建代理Key时，关联一个已存在的`RealApiKey` (`realKeyName`)。
    *   为每个代理Key配置一个或多个`RateLimitRule`。
    *   支持生成随机代理Key。
*   **配置同步**: 将这些配置通过ShenYu的配置同步机制（Nacos, Zookeeper, Etcd, Apollo, HTTP long polling）下发到数据平面网关实例。

**3.2. `shenyu-plugin-proxy-apikey-auth` (新建或增强现有认证插件)**

*   **职责**:
    1.  **提取代理Key**: 根据插件配置从请求中提取代理API Key。
    2.  **认证**:
        *   从ShenYu的动态配置中加载所有`ProxyApiKey`的定义。
        *   校验提取到的代理Key是否存在、`status`是否为"ENABLED"、是否过期。
        *   若无效，则拒绝请求 (HTTP 401/403)。
    3.  **加载策略**: 获取与该代理Key关联的`realKeyName`和`rateLimitRules`。
    4.  **上下文传递**: 将`proxyKey`, `realKeyName`以及将要使用的真实Key值（解密后）安全地传递到ShenYu的`ShenyuContext`或请求属性中，供后续插件使用。

**3.3. `shenyu-plugin-ratelimiter-proxy-apikey` (新建或增强现有`RateLimiterPlugin`)**

*   **职责**:
    1.  **获取限流目标和规则**: 从`ShenyuContext`或请求属性中获取当前代理Key (`proxyKey`) 及其关联的`rateLimitRules`。
    2.  **分布式限流检查**:
        *   对于每个`RateLimitRule`：
            *   构造Redis Key，例如: `shenyu:ratelimit:proxykey:<proxyKey>:<rule.id>:<timestamp_aligned_to_window_start>`.
            *   **请求数限流 (`type="REQUEST_COUNT"`)**:
                *   使用Redis的`INCR`原子增加计数。
                *   如果`INCR`后的值是1，则设置`EXPIRE`为`rule.timeWindowSeconds`。
                *   如果计数值超过`rule.limit`，则标记为超限。
            *   **Token数限流 (`type="TOKEN_COUNT"`)**:
                *   **预检 (可选)**: 如果是请求AI服务前检查，此时Token数未知，可以先按1个请求计数（如果也有请求数限流），或者此步骤跳过，在响应后再精确计数。
                *   **精确计数 (AI响应后)**: 需要在另一个插件（如`AIProxyPlugin`的后置处理）或此插件的响应处理阶段执行。此时已知实际消耗的Token。使用`INCRBY`原子增加Token消耗量。同样，首次增加时设置`EXPIRE`。
        *   如果任一规则超限，拒绝请求 (HTTP 429 Too Many Requests)，并可能在响应头中返回重试信息 (e.g., `Retry-After`).
    3.  **算法实现**:
        *   **`FIXED_WINDOW`**: 上述`INCR` + `EXPIRE` 方式。
        *   **`SLIDING_WINDOW_COUNTER` (推荐)**: 可以使用Redis Lua脚本实现更精确的滑动窗口。例如，为每个时间窗口维护多个小的时间片计数器，或者使用一个主计数器配合时间戳和Lua逻辑来模拟滑动。对于简单场景，固定窗口已足够。
*   **依赖**: Redis客户端 (e.g., Lettuce, Jedis)。

**3.4. `shenyu-plugin-ai-proxy` (或通用HTTP代理插件) 增强**

*   **职责**:
    1.  **获取真实Key**: 从`ShenyuContext`或请求属性中获取由`shenyu-plugin-proxy-apikey-auth`传递过来的真实API Key值。
    2.  **请求重写**:
        *   移除请求中的代理API Key (Header或参数)。
        *   将真实的API Key按AI服务提供商要求添加到请求中 (e.g., `Authorization: Bearer <real_key_value>`, `api-key: <real_key_value>`).
    3.  **转发请求**: 调用下游AI服务。
    4.  **响应处理 (Token计数)**:
        *   如果存在基于Token的限流规则，解析AI服务响应，提取实际消耗的Token数 (e.g., from OpenAI `usage.total_tokens`).
        *   将`proxyKey`和消耗的`tokens`存入`ShenyuContext`或请求属性中，供`shenyu-plugin-ratelimiter-proxy-apikey`的响应处理阶段使用，或直接调用限流引擎的Token更新接口（如果设计为分离的）。

**4. 核心流程**

**4.1. 配置阶段 (ShenYu Admin)**

1.  管理员在ShenYu Admin中创建/管理`RealApiKey` (加密存储真实Key)。
2.  管理员创建/管理`ProxyApiKey`，将其映射到`RealApiKey`，并配置`RateLimitRule`。
3.  配置通过ShenYu配置中心同步到所有网关实例。

**4.2. 请求处理阶段 (ShenYu网关插件链)**

假设插件链顺序: `proxy-apikey-auth` -> `ratelimiter-proxy-apikey` (请求阶段) -> `ai-proxy` -> `ratelimiter-proxy-apikey` (响应阶段，如果分离Token更新)

1.  **接收请求**: 客户端使用代理API Key发起请求。
2.  **`shenyu-plugin-proxy-apikey-auth`**:
    *   提取代理Key。
    *   验证代理Key有效性。
    *   加载其配置（真实Key名称，限流规则）。
    *   将信息存入`ShenyuContext`.
    *   若无效，拒绝。
3.  **`shenyu-plugin-ratelimiter-proxy-apikey` (请求阶段)**:
    *   获取代理Key和限流规则。
    *   **执行请求数限流检查**:
        *   调用Redis原子操作检查并增加计数。
        *   若超限，拒绝 (HTTP 429)。
    *   (可选) **执行Token数限流预检**: 如果有预估Token逻辑，并配置了预检，则执行。
4.  **`shenyu-plugin-ai-proxy`**:
    *   从`ShenyuContext`获取真实Key值（已解密）。
    *   重写请求，移除代理Key，添加真实Key。
    *   将请求转发到AI服务。
    *   接收AI服务响应。
    *   **解析Token用量**: 从响应中提取实际消耗的Token数。
    *   将消耗的Token数存入`ShenyuContext`.
5.  **`shenyu-plugin-ratelimiter-proxy-apikey` (响应阶段 - 仅当Token计数在此阶段处理)**:
    *   获取代理Key和之前消耗的Token数。
    *   **更新Token数限流计数**: 调用Redis原子操作增加Token消耗量。
    *   **注意**: 即使Token更新导致超限，此时请求已完成，通常不在此处拒绝。超限会影响该代理Key的下一个请求。
6.  **返回响应**: 网关将AI服务的响应返回给客户端。

**5. 分布式限流引擎 (Redis)**

*   **Redis Key设计**:
    *   `shenyu:rl:proxykey:<proxyKeyValue>:<ruleId>:<timeSlot>` (for Fixed Window or segments of Sliding Window)
    *   `proxyKeyValue`: 代理API Key的值。
    *   `ruleId`: 限流规则的唯一ID。
    *   `timeSlot`: 对齐到时间窗口起始的时间戳字符串 (e.g., `yyyyMMddHHmm` for minute window, `yyyyMMddHH` for hour window).
*   **原子操作**:
    *   使用`INCR`和`EXPIRE` (for fixed window request count).
    *   使用`INCRBY`和`EXPIRE` (for fixed window token count).
    *   使用Lua脚本实现更复杂的滑动窗口逻辑，确保原子性。
        *   例如，一个简单的滑动窗口计数器Lua脚本：
            ```lua
            -- ARGV[1]: limit, ARGV[2]: window_seconds, ARGV[3]: tokens_or_requests_to_consume
            local current_time = redis.call('TIME') -- [seconds, microseconds]
            local current_timestamp_seconds = tonumber(current_time[1])
            local key = KEYS[1]
            
            -- 清理旧的记录 (对于滑动窗口日志算法，或者维护多个时间片计数器时)
            -- redis.call('ZREMRANGEBYSCORE', key, 0, current_timestamp_seconds - window_seconds)
            
            -- 对于滑动窗口计数器（近似），通常使用一个key配合TTL
            local count = redis.call('GET', key)
            if not count then
              count = 0
            end
            count = tonumber(count)
            
            if count + tonumber(ARGV[3]) > tonumber(ARGV[1]) then
              return 0 -- Deny
            else
              local new_count = redis.call('INCRBY', key, tonumber(ARGV[3]))
              if new_count == tonumber(ARGV[3]) then -- First time setting this key in the window
                 redis.call('EXPIRE', key, ARGV[2])
              end
              return 1 -- Allow
            end
            ```
            *注：上述Lua脚本是一个简化示例，用于演示原子操作。实际滑动窗口实现可能更复杂，如维护多个时间片的计数器。对于简单的固定窗口计数器，直接使用`INCR`/`INCRBY` + `EXPIRE`更直接。*

**6. 增强功能与优化**

*   **监控与日志**:
    *   `shenyu-plugin-ratelimiter-proxy-apikey`应记录限流事件（哪个代理Key，哪个规则，是否通过）。
    *   `shenyu-plugin-proxy-apikey-auth`记录认证成功/失败。
    *   `shenyu-plugin-ai-proxy`记录Token消耗。
    *   指标：代理Key调用次数、Token用量、限流次数、活跃代理Key数量。
*   **安全性**:
    *   代理Key生成应使用强随机算法。
    *   真实Key加密存储的密钥管理。
*   **性能**:
    *   Redis操作保持高效，避免复杂Lua脚本的滥用。
    *   本地缓存（Guava Cache, Caffeine）`ProxyApiKey`配置，监听配置变更进行更新，减少对配置中心的频繁访问。但限流计数必须实时与Redis交互。
*   **错误处理**: 详细的错误码和信息返回给客户端。
*   **管理界面易用性**: ShenYu Admin提供清晰的界面进行配置管理。

**7. 技术栈与依赖**

*   **ShenYu**: 核心网关，插件体系，配置同步。
*   **Java**: 插件开发语言。
*   **Redis**: 分布式限流存储。
*   **Lettuce/Jedis**: Java Redis客户端。
*   **Jasypt/Spring Security Crypto**: (可选) 用于真实Key加密。
*   **JSON**: 配置数据格式。



**ShenYu AI模型调用语义缓存技术方案**

**1. 核心目标 (与笔记一致)**

1.  **显著减少Token消耗**
2.  **有效降低响应延迟**
3.  **提升回答一致性**
4.  **优化系统吞吐量**

**2. 系统整体架构 (融入ShenYu插件体系)**

我们将在ShenYu网关中通过一个专门的AI语义缓存插件（例如 `shenyu-plugin-ai-semantic-cache`）来实现此功能。该插件会在AI代理插件（如 `shenyu-plugin-ai-proxy`）调用实际AI服务之前或之后执行。

```mermaid
graph LR
    A[客户端请求] --> GW[ShenYu网关]

    subgraph GW
        P_AUTH[认证插件 e.g., ProxyApiKeyAuth] --> P_RATE_LIMIT[限流插件]
        P_RATE_LIMIT --> P_SEM_CACHE[AI语义缓存插件 shenyu-plugin-ai-semantic-cache]
        
        subgraph P_SEM_CACHE
            RI[请求解析与预处理]
            RI --> KGG[精确匹配键生成器]
            RI --> EG[嵌入向量生成器 (Spring AI EmbeddingClient)]
            
            KGG --> KV_STORE[KV缓存存储 (Redis/Ignite/In-Memory)]
            EG --> VDB_STORE[向量数据库 (PGVector/Milvus/OpenSearch)]
            
            KV_STORE --> SEM_EVAL[相似度评估与参数检查]
            VDB_STORE --> SEM_EVAL
            
            SEM_EVAL -- 精确/语义命中 --> HIT_HANDLER[缓存命中处理器]
            SEM_EVAL -- 未命中 --> MISS_FLAG[标记为未命中, 继续执行]
            
            HIT_HANDLER --> RESP_CACHE[直接返回缓存响应]
        end

        P_SEM_CACHE -- 未命中或后置处理 --> P_AI_PROXY[AI代理插件 shenyu-plugin-ai-proxy]
        
        subgraph P_AI_PROXY
            LLMA[LLM适配器 (Spring AI ChatClient)] --> EXT_LLM[外部LLM服务]
            EXT_LLM --> LLMA
        end

        P_AI_PROXY -- LLM响应 --> P_SEM_CACHE_POST[AI语义缓存插件 - 后置处理]

        subgraph P_SEM_CACHE_POST
             CACHE_FILLER[缓存填充处理器]
             CACHE_FILLER --> KV_STORE
             CACHE_FILLER --> VDB_STORE
             CACHE_FILLER --> RESP_LLM[返回LLM新响应]
        end
        
        RESP_CACHE --> FINAL_RESP[最终响应]
        RESP_LLM --> FINAL_RESP
    end
    
    FINAL_RESP --> A
```

**3. 核心组件详解 (ShenYu插件内部实现)**

**3.1. `AISemanticCachePlugin` (主插件)**

*   **执行时机**: 可以在AI代理插件之前执行（尝试读取缓存），并在其之后执行（填充缓存）。或者设计成一个包含前置和后置逻辑的单一插件。
*   **配置 (`PluginData`)**:
    *   `enabled`: `true` | `false`.
    *   `embeddingModelProvider`: (e.g., "openai", "ollama", "bge-local") - 指向Spring AI `EmbeddingClient`的配置。
    *   `embeddingModelName`: 具体的嵌入模型名称。
    *   `vectorStoreProvider`: (e.g., "pgvector", "milvus", "redis", "opensearch") - 指向向量存储的SPI实现。
    *   `kvStoreProvider`: (e.g., "redis", "ignite", "caffeine") - 指向KV存储的SPI实现。
    *   `similarityThreshold`: `float` (e.g., 0.9) - 语义相似度阈值。
    *   `topKCandidates`: `int` (e.g., 5) - 从向量库检索的候选数量。
    *   `exactMatchEnabled`: `true` | `false`.
    *   `parameterCompatibilityRules`: (定义哪些请求参数必须一致，哪些可以容忍)。
        *   `strictMatchParams`: `List<String>` (e.g., ["modelId", "systemPrompt"]).
        *   `flexibleParamsConfig`: `Map<String, Object>` (e.g., `{"temperature": {"maxDelta": 0.2}}`).
    *   `cacheTTLSeconds`: `long` (缓存默认TTL)。
    *   `tenantIsolationBy`: `String` (e.g., "header:X-Tenant-ID", "attribute:userId" - 从请求的哪个部分获取租户ID)。

**3.2. 请求解析与预处理 (Plugin内部逻辑)**

*   **职责**:
    *   从ShenYu `ServerWebExchange` 中提取关键信息：用户查询文本（通常是请求体），模型ID，temperature等AI调用参数（可能来自请求或AI代理插件的默认配置）。
    *   获取租户ID（如果配置了隔离）。
    *   **规范化查询文本**: 去除多余空格、标点符号归一化（可选，取决于嵌入模型）。
*   **输出**: 规范化查询文本、关键参数Map、租户ID。

**3.3. 精确匹配键生成器 (Plugin内部逻辑)**

*   **职责**: 如果`exactMatchEnabled`为true，根据规范化查询文本和`strictMatchParams`列表中的参数值，生成一个唯一的哈希键 (SHA256)。
*   **Key构成**: `hash(tenantId + ":" + queryText + ":" + sortedStrictParamValues)`.

**3.4. 嵌入向量生成器 (依赖Spring AI `EmbeddingClient`)**

*   **职责**: 将预处理后的查询文本转换为嵌入向量。
*   **实现**:
    *   插件通过SPI或直接配置获取Spring AI的`EmbeddingClient`实例。
    *   调用`embeddingClient.embed(normalizedQueryText)`。
    *   错误处理和重试应由`EmbeddingClient`或其封装层处理。

**3.5. `VectorStoreService` (ShenYu SPI)**

*   **接口定义**:
    *   `void addVector(String id, float[] vector, Map<String, Object> metadata)`
    *   `List<VectorSearchResult> searchSimilar(float[] queryVector, int topK, Map<String, Object> filterMetadata)`
    *   `void deleteVector(String id)`
*   **`VectorSearchResult`**: 包含`id`, `score`, `metadata`.
*   **实现**:
    *   `PgVectorStoreServiceImpl`: 使用JDBC/JPA与PostgreSQL (pgvector) 交互。元数据过滤通过WHERE子句实现。
    *   `MilvusVectorStoreServiceImpl`: 使用Milvus SDK。
    *   `RedisVectorStoreServiceImpl`: 使用Redis Stack (RediSearch + RedisVL) 的向量搜索功能。
    *   ... 其他实现。
*   **元数据**: 至少包含关联的`kvCacheId`和`tenantId`。

**3.6. `KeyValueStoreService` (ShenYu SPI)**

*   **接口定义**:
    *   `Optional<CachedResponseEntity> get(String key)`
    *   `void put(String key, CachedResponseEntity value, long ttlSeconds)`
    *   `void delete(String key)`
*   **`CachedResponseEntity`**:
    *   `id`: `String` (唯一ID，可作为`kvCacheId`关联到向量元数据)。
    *   `exactMatchKey`: `String` (可选)。
    *   `queryTextNormalized`: `String`.
    *   `requestParamsSnapshot`: `Map<String, Object>` (缓存时的请求参数快照)。
    *   `llmResponse`: `String` or `byte[]` (AI模型的原始响应体)。
    *   `responseHeaders`: `Map<String, List<String>>` (AI模型的响应头)。
    *   `embeddingVectorId`: `String` (关联到`VectorStoreService`中的向量ID，可选)。
    *   `createdAt`, `lastAccessedAt`, `hitCount`.
    *   `tenantId`: `String`.
*   **实现**:
    *   `RedisKeyValueStoreServiceImpl`: 使用Redis (String/Hash) 存储序列化的`CachedResponseEntity`.
    *   `CaffeineKeyValueStoreServiceImpl`: 使用Caffeine作为本地内存缓存。
    *   `IgniteKeyValueStoreServiceImpl`: 使用Apache Ignite。

**3.7. 相似度评估与参数检查 (Plugin内部逻辑)**

*   **职责**:
    *   接收新查询的嵌入向量`V_new`和从`VectorStoreService`检索到的Top-K候选结果。
    *   计算余弦相似度。
    *   对每个满足`similarityThreshold`的候选：
        *   从其`VectorSearchResult.metadata`中获取`kvCacheId`.
        *   调用`KeyValueStoreService.get(kvCacheId)`获取`CachedResponseEntity`.
        *   比较`CachedResponseEntity.requestParamsSnapshot`与当前请求参数，依据`parameterCompatibilityRules`判断是否兼容。
*   **输出**: 最佳匹配的`CachedResponseEntity`，或null（未命中）。

**3.8. 缓存命中/未命中处理器 (Plugin内部逻辑)**

*   **命中处理器**:
    *   更新`CachedResponseEntity`的`lastAccessedAt`, `hitCount` (调用`KeyValueStoreService.put`覆盖)。
    *   将`CachedResponseEntity.llmResponse`和`responseHeaders`构建成`ServerHttpResponse`，并直接返回，中断后续插件执行。
*   **未命中标记**: 在`ServerWebExchange`的Attributes中设置一个标记，表示缓存未命中，让后续插件（如AI代理插件）继续执行。

**3.9. 缓存填充处理器 (Plugin内部逻辑 - 在AI代理插件执行后)**

*   **触发条件**: `ServerWebExchange`的Attributes中包含“缓存未命中”标记，并且AI代理插件成功返回了新的LLM响应。
*   **职责**:
    1.  获取原始请求的规范化查询、参数、租户ID（可从前置处理阶段缓存到Attributes）。
    2.  获取LLM的新响应体和响应头。
    3.  如果之前未生成嵌入向量（例如，精确匹配优先且未命中），则调用嵌入生成器生成`V_new`。
    4.  创建`CachedResponseEntity`实例。
    5.  **写入KV存储**: 调用`KeyValueStoreService.put()`.
    6.  **写入向量存储**:
        *   如果`V_new`已生成，调用`VectorStoreService.addVector()`，元数据中包含刚存入KV存储的`CachedResponseEntity.id`和`tenantId`.
        *   `CachedResponseEntity`中保存`embeddingVectorId` (如果向量ID与KV的ID不同)。
    7.  将新的LLM响应继续传递给客户端。

**4. 核心缓存工作流程 (在 `AISemanticCachePlugin` 中)**

1.  **前置处理**:
    *   插件被调用，解析请求，预处理查询，获取参数和租户ID。
    *   **精确匹配尝试 (如果启用)**:
        *   生成`exactMatchKey`.
        *   调用`KeyValueStoreService.get(exactMatchKey)`.
        *   若命中且参数兼容（通常精确匹配要求所有相关参数严格一致），执行缓存命中处理器，返回响应，流程结束。
    *   **语义匹配尝试 (如果精确未命中或未启用)**:
        *   生成查询嵌入向量`V_new`.
        *   调用`VectorStoreService.searchSimilar(V_new, topK, filterByTenantId)`.
        *   对结果进行相似度评估和参数兼容性检查。
        *   若找到有效命中，执行缓存命中处理器，返回响应，流程结束。
    *   **缓存未命中**: 在`ServerWebExchange`的Attributes中设置“缓存未命中”标记。插件将请求传递给插件链中的下一个插件（通常是`AIProxyPlugin`）。

2.  **后置处理 (在`AIProxyPlugin`成功返回LLM响应后)**:
    *   插件再次被调用（或其后置逻辑部分）。
    *   检查Attributes中的“缓存未命中”标记。
    *   如果标记存在，执行缓存填充处理器的逻辑，将新的问答对、向量等存入相应存储。
    *   将LLM响应返回给客户端。

**5. AI提供商原生缓存集成策略 (笔记内容适用)**

*   可以在`AISemanticCachePlugin`的配置中增加一个选项，如`preferProviderCache: true/false`.
*   如果`preferProviderCache`为true，并且`AIProxyPlugin`知道当前调用的AI模型支持原生缓存，则`AISemanticCachePlugin`在前置处理阶段可以跳过自建缓存查找，直接让`AIProxyPlugin`优先尝试提供商缓存。
*   `AIProxyPlugin`在调用Spring AI `ChatClient`时，需要根据提供商的API规范传递缓存控制参数。
*   如果提供商缓存命中，`AIProxyPlugin`返回结果。`AISemanticCachePlugin`的后置处理器可以选择是否将被提供商缓存的结果也异步写入自建缓存（用于分析或备用）。
*   如果提供商缓存未命中，则`AIProxyPlugin`得到新结果，`AISemanticCachePlugin`的后置处理器按正常流程填充自建缓存。

**6. 缓存策略与管理 (笔记内容适用)**

*   **TTL, LRU/LFU (用于KV和向量存储)**: `KeyValueStoreService`和`VectorStoreService`的实现应考虑这些策略。例如，Redis自带TTL和LRU/LFU。PGVector等可能需要外部任务或DB机制来清理。
*   **缓存更新与失效**: 主要依赖TTL。主动失效需要更复杂的外部触发机制，初期可不实现。嵌入模型或LLM模型更新时，可能需要策略性地清空或部分刷新缓存（例如，通过管理API或基于时间戳）。
*   **参数敏感度与兼容性**: 在插件配置 (`PluginData`) 中详细定义。
*   **多租户隔离**: 通过`tenantId`在KV键、向量元数据中实现。

**7. 关键技术选型考量 (笔记内容适用，结合ShenYu和Spring AI)**

*   **嵌入模型**: 通过Spring AI `EmbeddingClient` SPI选择和配置。
*   **向量数据库**: 实现`VectorStoreService` SPI。初期可以从`pgvector`或基于Redis的方案开始。
*   **KV缓存存储**: 实现`KeyValueStoreService` SPI。Redis是常用选择。
*   **编程语言**: Java (ShenYu插件)。

**9. 总结与挑战 

**主要ShenYu特定挑战**:

*   **插件执行顺序与数据传递**: 精确控制缓存插件与AI代理插件的执行顺序，以及它们之间的数据共享（通过`ServerWebExchange.getAttributes()`）。
*   **SPI设计**: `VectorStoreService`和`KeyValueStoreService`的SPI接口需要设计得通用且高效。
*   **Spring AI集成**: 确保`EmbeddingClient`等Spring AI组件能在ShenYu插件环境中正确初始化和使用。可能需要为插件创建一个小型的Spring ApplicationContext。
*   **异步处理**: 嵌入生成、向量存储写入等耗时操作应异步化，避免阻塞网关的事件循环线程。

此方案利用ShenYu的插件化架构和SPI机制，结合Spring AI的能力，构建了一个可插拔、可配置的AI语义缓存系统，旨在为通过ShenYu网关的AI调用带来显著的成本和性能优化。